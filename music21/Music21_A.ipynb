{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The program is referenced and modified from:\n",
    "> https://towardsdatascience.com/how-to-generate-music-using-a-lstm-neural-network-in-keras-68786834d4c5  \n",
    "\n",
    "Reference article explaining how to improve the program:\n",
    "> https://david-exiga.medium.com/music-generation-using-lstm-neural-networks-44f6780a4c5  \n",
    "\n",
    "Additional Chinese program explanation:\n",
    "> https://github.com/xitu/gold-miner/blob/master/TODO1/how-to-generate-music-using-a-lstm-neural-network-in-keras.md"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: music21 in /usr/local/lib/python3.10/dist-packages (9.3.0)\n",
      "Requirement already satisfied: keras in /usr/local/lib/python3.10/dist-packages (3.7.0)\n",
      "Requirement already satisfied: tensorflow[and-cuda] in /usr/local/lib/python3.10/dist-packages (2.18.0)\n",
      "Requirement already satisfied: chardet in /usr/local/lib/python3.10/dist-packages (from music21) (5.2.0)\n",
      "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from music21) (1.4.2)\n",
      "Requirement already satisfied: jsonpickle in /usr/local/lib/python3.10/dist-packages (from music21) (4.0.0)\n",
      "Requirement already satisfied: matplotlib in /usr/local/lib/python3.10/dist-packages (from music21) (3.9.3)\n",
      "Requirement already satisfied: more-itertools in /usr/lib/python3/dist-packages (from music21) (8.10.0)\n",
      "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from music21) (1.26.3)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from music21) (2.31.0)\n",
      "Requirement already satisfied: webcolors>=1.5 in /usr/local/lib/python3.10/dist-packages (from music21) (1.13)\n",
      "Requirement already satisfied: absl-py in /usr/local/lib/python3.10/dist-packages (from keras) (2.1.0)\n",
      "Requirement already satisfied: rich in /usr/local/lib/python3.10/dist-packages (from keras) (13.9.4)\n",
      "Requirement already satisfied: namex in /usr/local/lib/python3.10/dist-packages (from keras) (0.0.8)\n",
      "Requirement already satisfied: h5py in /usr/local/lib/python3.10/dist-packages (from keras) (3.12.1)\n",
      "Requirement already satisfied: optree in /usr/local/lib/python3.10/dist-packages (from keras) (0.13.1)\n",
      "Requirement already satisfied: ml-dtypes in /usr/local/lib/python3.10/dist-packages (from keras) (0.4.1)\n",
      "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from keras) (23.2)\n",
      "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow[and-cuda]) (1.6.3)\n",
      "Requirement already satisfied: flatbuffers>=24.3.25 in /usr/local/lib/python3.10/dist-packages (from tensorflow[and-cuda]) (24.3.25)\n",
      "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow[and-cuda]) (0.6.0)\n",
      "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow[and-cuda]) (0.2.0)\n",
      "Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow[and-cuda]) (18.1.1)\n",
      "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.10/dist-packages (from tensorflow[and-cuda]) (3.4.0)\n",
      "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<6.0.0dev,>=3.20.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow[and-cuda]) (5.29.0)\n",
      "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from tensorflow[and-cuda]) (69.0.3)\n",
      "Requirement already satisfied: six>=1.12.0 in /usr/lib/python3/dist-packages (from tensorflow[and-cuda]) (1.16.0)\n",
      "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow[and-cuda]) (2.5.0)\n",
      "Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.10/dist-packages (from tensorflow[and-cuda]) (4.9.0)\n",
      "Requirement already satisfied: wrapt>=1.11.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow[and-cuda]) (1.17.0)\n",
      "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow[and-cuda]) (1.68.1)\n",
      "Requirement already satisfied: tensorboard<2.19,>=2.18 in /usr/local/lib/python3.10/dist-packages (from tensorflow[and-cuda]) (2.18.0)\n",
      "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow[and-cuda]) (0.37.1)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.5.3.2 in /usr/local/lib/python3.10/dist-packages (from tensorflow[and-cuda]) (12.5.3.2)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.5.82 in /usr/local/lib/python3.10/dist-packages (from tensorflow[and-cuda]) (12.5.82)\n",
      "Requirement already satisfied: nvidia-cuda-nvcc-cu12==12.5.82 in /usr/local/lib/python3.10/dist-packages (from tensorflow[and-cuda]) (12.5.82)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.5.82 in /usr/local/lib/python3.10/dist-packages (from tensorflow[and-cuda]) (12.5.82)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.5.82 in /usr/local/lib/python3.10/dist-packages (from tensorflow[and-cuda]) (12.5.82)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==9.3.0.75 in /usr/local/lib/python3.10/dist-packages (from tensorflow[and-cuda]) (9.3.0.75)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.2.3.61 in /usr/local/lib/python3.10/dist-packages (from tensorflow[and-cuda]) (11.2.3.61)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.6.82 in /usr/local/lib/python3.10/dist-packages (from tensorflow[and-cuda]) (10.3.6.82)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.6.3.83 in /usr/local/lib/python3.10/dist-packages (from tensorflow[and-cuda]) (11.6.3.83)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.5.1.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow[and-cuda]) (12.5.1.3)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.10/dist-packages (from tensorflow[and-cuda]) (2.21.5)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12==12.5.82 in /usr/local/lib/python3.10/dist-packages (from tensorflow[and-cuda]) (12.5.82)\n",
      "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from astunparse>=1.6.0->tensorflow[and-cuda]) (0.42.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->music21) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->music21) (3.6)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->music21) (2.2.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->music21) (2024.2.2)\n",
      "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.19,>=2.18->tensorflow[and-cuda]) (3.7)\n",
      "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.19,>=2.18->tensorflow[and-cuda]) (0.7.2)\n",
      "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.19,>=2.18->tensorflow[and-cuda]) (3.1.3)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->music21) (1.3.1)\n",
      "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib->music21) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->music21) (4.55.0)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->music21) (1.4.7)\n",
      "Requirement already satisfied: pillow>=8 in /usr/local/lib/python3.10/dist-packages (from matplotlib->music21) (10.2.0)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in /usr/lib/python3/dist-packages (from matplotlib->music21) (2.4.7)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib->music21) (2.8.2)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich->keras) (3.0.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich->keras) (2.17.2)\n",
      "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py>=2.2.0->rich->keras) (0.1.2)\n",
      "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.10/dist-packages (from werkzeug>=1.0.1->tensorboard<2.19,>=2.18->tensorflow[and-cuda]) (2.1.5)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.0\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.3.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython -m pip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# Install dependency \n",
    "# music21 Introduction: https://juejin.cn/post/7063827463058489352\n",
    "! pip install music21 keras tensorflow[and-cuda] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For reading files\n",
    "import glob\n",
    "# array processing\n",
    "import numpy\n",
    "from matplotlib import pyplot\n",
    "# keras for building deep learning model\n",
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, TimeDistributed\n",
    "from keras.layers import Dropout\n",
    "from keras.layers import LSTM\n",
    "from keras.layers import Activation\n",
    "from keras.layers import BatchNormalization as BatchNorm\n",
    "from keras.utils import to_categorical\n",
    "from keras.callbacks import ModelCheckpoint\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get all notes and chords from midi files in a directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parsing ./midi_songs/Bohemian Rhapsody.mid\n",
      "Parsing ./midi_songs/Never-Gonna-Give-You-Up.mid\n",
      "Parsing ./midi_songs/Tokyo Ghoul - Unravel.mid\n",
      "Parsing ./midi_songs/One Night In Tokyo.mid\n",
      "Parsing ./midi_songs/Linkin Park - One Step Closer.mid\n",
      "Parsing ./midi_songs/The-Final-Countdown.mid\n",
      "Parsing ./midi_songs/Daft Punk - Aerodynamic.mid\n",
      "Total notes and chords extracted: 897\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Extract all notes and chords from MIDI files in a directory.\n",
    "\n",
    "This script uses music21 to process MIDI files. It reads all `.mid` files in the \n",
    "specified directory, extracts notes and chords, and appends them to the `notes` list.\n",
    "\"\"\"\n",
    "\n",
    "# Import necessary modules from music21 for MIDI processing\n",
    "from music21 import converter, instrument, note, chord\n",
    "import glob\n",
    "\n",
    "# List to store all notes and chords from the MIDI files\n",
    "notes = []\n",
    "\n",
    "# Specify the path to the MIDI files (modify as needed)\n",
    "midi_path = \"./midi_songs/*.mid\"\n",
    "\n",
    "# Loop through all MIDI files in the specified directory\n",
    "for file in glob.glob(midi_path):\n",
    "    print(f\"Parsing {file}\")\n",
    "    \n",
    "    # Parse the MIDI file using music21\n",
    "    midi = converter.parse(file)\n",
    "    \n",
    "    # Initialize a variable to hold notes and chords to be parsed\n",
    "    notes_to_parse = None\n",
    "\n",
    "    try:\n",
    "        # If the MIDI file contains instrument parts, extract the first part\n",
    "        s2 = instrument.partitionByInstrument(midi)\n",
    "        notes_to_parse = s2.parts[0].recurse()  # Access notes recursively\n",
    "    except AttributeError:\n",
    "        # If no instrument parts, use the flat structure to access notes\n",
    "        notes_to_parse = midi.flat.notes\n",
    "\n",
    "    # Extract notes and chords from the parsed MIDI data\n",
    "    for element in notes_to_parse:\n",
    "        if isinstance(element, note.Note):\n",
    "            # If the element is a Note, extract its pitch as a string\n",
    "            notes.append(str(element.pitch))\n",
    "        elif isinstance(element, chord.Chord):\n",
    "            # If the element is a Chord, extract its normal order as a string\n",
    "            notes.append('.'.join(str(n) for n in element.normalOrder))\n",
    "\n",
    "# Output the total number of notes and chords extracted\n",
    "print(f\"Total notes and chords extracted: {len(notes)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare input and output for neural network use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===== Explanation of Variables =====\n",
      "\n",
      "notes: A list containing all the musical notes as strings.\n",
      "Total number of notes in the score: 897\n",
      "Total unique note types in the score: 51\n",
      "Unique note types: ['0', '0.5', '2.7', '3.7', '3.9', '5.10', '5.8', '7.0', '8.0', '9.0', 'A2', 'A4', 'A5', 'A6', 'B-4', 'B-5', 'B1', 'B2', 'B3', 'B4', 'B5', 'C#2', 'C#4', 'C#5', 'C1', 'C3', 'C4', 'C5', 'C6', 'D2', 'D5', 'D6', 'E-5', 'E-6', 'E2', 'E4', 'E5', 'E6', 'F#2', 'F#4', 'F#5', 'F#6', 'F2', 'F5', 'F6', 'G#2', 'G#4', 'G#5', 'G4', 'G5', 'G6']\n",
      "Mapping of note types to IDs: {'0': 0, '0.5': 1, '2.7': 2, '3.7': 3, '3.9': 4, '5.10': 5, '5.8': 6, '7.0': 7, '8.0': 8, '9.0': 9, 'A2': 10, 'A4': 11, 'A5': 12, 'A6': 13, 'B-4': 14, 'B-5': 15, 'B1': 16, 'B2': 17, 'B3': 18, 'B4': 19, 'B5': 20, 'C#2': 21, 'C#4': 22, 'C#5': 23, 'C1': 24, 'C3': 25, 'C4': 26, 'C5': 27, 'C6': 28, 'D2': 29, 'D5': 30, 'D6': 31, 'E-5': 32, 'E-6': 33, 'E2': 34, 'E4': 35, 'E5': 36, 'E6': 37, 'F#2': 38, 'F#4': 39, 'F#5': 40, 'F#6': 41, 'F2': 42, 'F5': 43, 'F6': 44, 'G#2': 45, 'G#4': 46, 'G#5': 47, 'G4': 48, 'G5': 49, 'G6': 50}\n",
      "\n",
      "===================\n",
      "\n",
      "Total notes: 897\n",
      "Each 100 notes are converted into a training data set.\n",
      "network_input: 797 sequences, each containing 100 numeric IDs.\n",
      "network_output: 797 numeric IDs, each corresponding to the next note in the sequence.\n",
      "\n",
      "===================\n",
      "\n",
      "Notes from index sequence_length - 10 to sequence_length:\n",
      "['B-4', 'B-4', 'E6', 'E6', 'A6', 'E6', 'E6', 'A6', 'E6', 'E6']\n",
      "Corresponding numeric IDs:\n",
      "[14, 14, 37, 37, 13, 37, 37, 13, 37, 37]\n",
      "\n",
      "Last 10 IDs of the 0th sequence in network_input: [14, 14, 37, 37, 13, 37, 37, 13, 37, 37]\n",
      "Last 10 IDs of the 1st sequence in network_input: [14, 37, 37, 13, 37, 37, 13, 37, 37, 33]\n",
      "Last 10 IDs of the 2nd sequence in network_input: [37, 37, 13, 37, 37, 13, 37, 37, 33, 33]\n",
      "First three outputs in network_output: [33, 33, 37]\n",
      "\n",
      "===== After Reshaping Data =====\n",
      "\n",
      "normalized_input.shape: (797, 100, 1)\n",
      "network_output.shape: (797, 51)\n"
     ]
    }
   ],
   "source": [
    "# Prepare inputs and outputs for neural network\n",
    "\n",
    "# Get the number of unique note names\n",
    "n_vocab = len(set(notes))\n",
    "\n",
    "# Get the sorted list of unique note names\n",
    "pitchnames = sorted(set(item for item in notes))\n",
    "\n",
    "# Create a dictionary mapping each note to a corresponding numeric ID (e.g., C4 -> 25)\n",
    "note_to_int = {note: number for number, note in enumerate(pitchnames)}\n",
    "\n",
    "print(\"\\n===== Explanation of Variables =====\\n\")\n",
    "print(\"notes: A list containing all the musical notes as strings.\")\n",
    "print(f\"Total number of notes in the score: {len(notes)}\")\n",
    "print(f\"Total unique note types in the score: {n_vocab}\")\n",
    "print(f\"Unique note types: {pitchnames}\")\n",
    "print(f\"Mapping of note types to IDs: {note_to_int}\")\n",
    "\n",
    "# Length of the input sequence for training\n",
    "sequence_length = 100\n",
    "\n",
    "# Create input and output sequences\n",
    "network_input = []\n",
    "network_output = []\n",
    "\n",
    "# Ensure the notes list is long enough for the sequence length\n",
    "if len(notes) > sequence_length:\n",
    "    for i in range(len(notes) - sequence_length):\n",
    "        # Input sequence of notes\n",
    "        sequence_in = notes[i:i + sequence_length]\n",
    "        # Corresponding output note\n",
    "        sequence_out = notes[i + sequence_length]\n",
    "\n",
    "        # Convert input sequence to numeric format\n",
    "        network_input.append([note_to_int[char] for char in sequence_in])\n",
    "        # Convert output note to numeric format\n",
    "        network_output.append(note_to_int[sequence_out])\n",
    "\n",
    "    print(\"\\n===================\\n\")\n",
    "    print(f\"Total notes: {len(notes)}\")\n",
    "    print(f\"Each {sequence_length} notes are converted into a training data set.\")\n",
    "    print(f\"network_input: {len(network_input)} sequences, each containing {len(network_input[0])} numeric IDs.\")\n",
    "    print(f\"network_output: {len(network_output)} numeric IDs, each corresponding to the next note in the sequence.\")\n",
    "    print(\"\\n===================\\n\")\n",
    "    print(\"Notes from index sequence_length - 10 to sequence_length:\")\n",
    "    print(notes[sequence_length-10:sequence_length])\n",
    "    print(\"Corresponding numeric IDs:\")\n",
    "    print([note_to_int[char] for char in notes[sequence_length-10:sequence_length]])\n",
    "    print(\"\")\n",
    "    print(f\"Last 10 IDs of the 0th sequence in network_input: {network_input[0][sequence_length-10:sequence_length]}\")\n",
    "    print(f\"Last 10 IDs of the 1st sequence in network_input: {network_input[1][sequence_length-10:sequence_length]}\")\n",
    "    print(f\"Last 10 IDs of the 2nd sequence in network_input: {network_input[2][sequence_length-10:sequence_length]}\")\n",
    "    print(\"First three outputs in network_output:\", network_output[0:3])\n",
    "\n",
    "    # Number of patterns\n",
    "    n_patterns = len(network_input)\n",
    "\n",
    "    # Reshape input for LSTM compatibility\n",
    "    normalized_input = numpy.reshape(network_input, (n_patterns, sequence_length, 1))\n",
    "\n",
    "    # Normalize input\n",
    "    normalized_input = normalized_input / float(n_vocab)\n",
    "\n",
    "    # Convert output to categorical format\n",
    "    network_output = to_categorical(network_output, n_vocab)\n",
    "\n",
    "    print(\"\\n===== After Reshaping Data =====\\n\")\n",
    "    print(\"normalized_input.shape:\", normalized_input.shape)\n",
    "    print(\"network_output.shape:\", network_output.shape)\n",
    "\n",
    "else:\n",
    "    print(f\"Error: The notes list must be longer than the sequence length ({sequence_length}).\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create the structure of a neural network \n",
    "### LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential_5\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"sequential_5\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ lstm_15 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LSTM</span>)                  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">100</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)       │     <span style=\"color: #00af00; text-decoration-color: #00af00\">1,052,672</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ lstm_16 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LSTM</span>)                  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">100</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)       │     <span style=\"color: #00af00; text-decoration-color: #00af00\">2,099,200</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ lstm_17 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LSTM</span>)                  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)            │     <span style=\"color: #00af00; text-decoration-color: #00af00\">2,099,200</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ batch_normalization_10          │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)            │         <span style=\"color: #00af00; text-decoration-color: #00af00\">2,048</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_10 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)            │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)            │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_10 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)            │       <span style=\"color: #00af00; text-decoration-color: #00af00\">131,328</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ activation_10 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Activation</span>)      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)            │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ batch_normalization_11          │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)            │         <span style=\"color: #00af00; text-decoration-color: #00af00\">1,024</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_11 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)            │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)            │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_11 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">51</span>)             │        <span style=\"color: #00af00; text-decoration-color: #00af00\">13,107</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ activation_11 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Activation</span>)      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">51</span>)             │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ lstm_15 (\u001b[38;5;33mLSTM\u001b[0m)                  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m100\u001b[0m, \u001b[38;5;34m512\u001b[0m)       │     \u001b[38;5;34m1,052,672\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ lstm_16 (\u001b[38;5;33mLSTM\u001b[0m)                  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m100\u001b[0m, \u001b[38;5;34m512\u001b[0m)       │     \u001b[38;5;34m2,099,200\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ lstm_17 (\u001b[38;5;33mLSTM\u001b[0m)                  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m512\u001b[0m)            │     \u001b[38;5;34m2,099,200\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ batch_normalization_10          │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m512\u001b[0m)            │         \u001b[38;5;34m2,048\u001b[0m │\n",
       "│ (\u001b[38;5;33mBatchNormalization\u001b[0m)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_10 (\u001b[38;5;33mDropout\u001b[0m)            │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m512\u001b[0m)            │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_10 (\u001b[38;5;33mDense\u001b[0m)                │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m)            │       \u001b[38;5;34m131,328\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ activation_10 (\u001b[38;5;33mActivation\u001b[0m)      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m)            │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ batch_normalization_11          │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m)            │         \u001b[38;5;34m1,024\u001b[0m │\n",
       "│ (\u001b[38;5;33mBatchNormalization\u001b[0m)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_11 (\u001b[38;5;33mDropout\u001b[0m)            │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m)            │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_11 (\u001b[38;5;33mDense\u001b[0m)                │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m51\u001b[0m)             │        \u001b[38;5;34m13,107\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ activation_11 (\u001b[38;5;33mActivation\u001b[0m)      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m51\u001b[0m)             │             \u001b[38;5;34m0\u001b[0m │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">5,398,579</span> (20.59 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m5,398,579\u001b[0m (20.59 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">5,397,043</span> (20.59 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m5,397,043\u001b[0m (20.59 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">1,536</span> (6.00 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m1,536\u001b[0m (6.00 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\"\"\"\n",
    "Create the structure of the neural network using LSTM layers.\n",
    "\n",
    "This model is designed for sequence prediction tasks, leveraging the strengths of\n",
    "LSTM layers for processing sequential data like music notes and chords.\n",
    "\"\"\"\n",
    "\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense, Dropout, Activation, BatchNormalization\n",
    "from tensorflow.keras.optimizers import RMSprop\n",
    "\n",
    "# Define the model\n",
    "model = Sequential()\n",
    "\n",
    "# Add the first LSTM layer with return_sequences=True for stacked LSTM\n",
    "model.add(LSTM(\n",
    "    512,  # Number of units in the LSTM layer\n",
    "    input_shape=(normalized_input.shape[1], normalized_input.shape[2]),  # Input shape\n",
    "    recurrent_dropout=0.1,  # Dropout for recurrent connections\n",
    "    return_sequences=True  # Return sequences for stacking LSTM layers\n",
    "))\n",
    "\n",
    "# Add the second LSTM layer\n",
    "model.add(LSTM(512, return_sequences=True, recurrent_dropout=0.1))\n",
    "\n",
    "# Add the third LSTM layer (no return_sequences since it's the last LSTM layer)\n",
    "model.add(LSTM(512))\n",
    "\n",
    "# Add Batch Normalization\n",
    "model.add(BatchNormalization())\n",
    "\n",
    "# Add a Dropout layer to reduce overfitting\n",
    "model.add(Dropout(0.1))\n",
    "\n",
    "# Add a Dense layer with 256 units and ReLU activation\n",
    "model.add(Dense(256))\n",
    "model.add(Activation('relu'))\n",
    "\n",
    "# Add Batch Normalization\n",
    "model.add(BatchNormalization())\n",
    "\n",
    "# Add another Dropout layer\n",
    "model.add(Dropout(0.1))\n",
    "\n",
    "# Add the output layer with softmax activation\n",
    "model.add(Dense(n_vocab))  # n_vocab is the number of unique notes/chords\n",
    "model.add(Activation('softmax'))\n",
    "\n",
    "# Compile the model with categorical crossentropy loss and RMSprop optimizer\n",
    "model.compile(\n",
    "    loss='categorical_crossentropy', \n",
    "    optimizer=RMSprop(learning_rate=0.001)  # Specify learning rate\n",
    ")\n",
    "\n",
    "# Display the model summary\n",
    "model.summary()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train a neural network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 153ms/step - loss: 4.3516\n",
      "Epoch 1: loss improved from inf to 4.18886, saving model to best_model.keras\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 188ms/step - loss: 4.3312\n",
      "Epoch 2/50\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 161ms/step - loss: 4.0272\n",
      "Epoch 2: loss improved from 4.18886 to 3.85156, saving model to best_model.keras\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 193ms/step - loss: 4.0052\n",
      "Epoch 3/50\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 166ms/step - loss: 3.6239\n",
      "Epoch 3: loss improved from 3.85156 to 3.56712, saving model to best_model.keras\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 197ms/step - loss: 3.6168\n",
      "Epoch 4/50\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 156ms/step - loss: 3.5192\n",
      "Epoch 4: loss improved from 3.56712 to 3.46361, saving model to best_model.keras\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 184ms/step - loss: 3.5123\n",
      "Epoch 5/50\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 166ms/step - loss: 3.3946\n",
      "Epoch 5: loss improved from 3.46361 to 3.36386, saving model to best_model.keras\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 195ms/step - loss: 3.3907\n",
      "Epoch 6/50\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 160ms/step - loss: 3.2957\n",
      "Epoch 6: loss did not improve from 3.36386\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 164ms/step - loss: 3.3048\n",
      "Epoch 7/50\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 167ms/step - loss: 4.6017\n",
      "Epoch 7: loss did not improve from 3.36386\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 170ms/step - loss: 4.6004\n",
      "Epoch 8/50\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 168ms/step - loss: 4.5054\n",
      "Epoch 8: loss did not improve from 3.36386\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 172ms/step - loss: 4.5065\n",
      "Epoch 9/50\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 163ms/step - loss: 4.4718\n",
      "Epoch 9: loss did not improve from 3.36386\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 167ms/step - loss: 4.4702\n",
      "Epoch 10/50\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 163ms/step - loss: 4.3158\n",
      "Epoch 10: loss did not improve from 3.36386\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 168ms/step - loss: 4.3188\n",
      "Epoch 11/50\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 167ms/step - loss: 4.2458\n",
      "Epoch 11: loss did not improve from 3.36386\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 171ms/step - loss: 4.2492\n",
      "Epoch 12/50\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 166ms/step - loss: 4.1412\n",
      "Epoch 12: loss did not improve from 3.36386\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 170ms/step - loss: 4.1323\n",
      "Epoch 13/50\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 162ms/step - loss: 4.0538\n",
      "Epoch 13: loss did not improve from 3.36386\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 166ms/step - loss: 4.0425\n",
      "Epoch 14/50\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 166ms/step - loss: 3.8349\n",
      "Epoch 14: loss did not improve from 3.36386\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 171ms/step - loss: 3.8277\n",
      "Epoch 15/50\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 162ms/step - loss: 3.6222\n",
      "Epoch 15: loss did not improve from 3.36386\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 166ms/step - loss: 3.6067\n",
      "Training complete! The best model has been saved as 'best_model.keras'.\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Train the neural network for generating music sequences.\n",
    "\n",
    "This process adjusts the weights of the model based on the provided input\n",
    "and output, enabling it to learn patterns in the musical dataset. Only the best model\n",
    "based on training loss will be saved.\n",
    "\"\"\"\n",
    "\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping\n",
    "\n",
    "# Define a callback to save the best model based on training loss\n",
    "callbacks = [\n",
    "    # Save only the best model based on minimum loss\n",
    "    ModelCheckpoint(\n",
    "        filepath='best_model.keras',  # Filepath to save the best model in .keras format\n",
    "        monitor='loss',               # Monitor training loss for improvement\n",
    "        save_best_only=True,          # Save only the best model weights\n",
    "        mode='min',                   # Minimize the monitored value (loss)\n",
    "        verbose=1\n",
    "    ),\n",
    "    # Stop training early if the loss stagnates\n",
    "    EarlyStopping(\n",
    "        monitor='loss', \n",
    "        patience=10,                  # Wait for 10 epochs of no improvement\n",
    "        restore_best_weights=True     # Load the best weights when stopping\n",
    "    )\n",
    "]\n",
    "\n",
    "# Train the model\n",
    "history = model.fit(\n",
    "    normalized_input,  # Input data\n",
    "    network_output,    # Expected output\n",
    "    epochs=50,         # Total number of training epochs\n",
    "    batch_size=128,    # Size of each training batch\n",
    "    callbacks=callbacks,  # Attach callbacks\n",
    "    verbose=1          # Print progress during training\n",
    ")\n",
    "\n",
    "print(\"Training complete! The best model has been saved as 'best_model.keras'.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based on the selected note starting point, predict the next note from the neural network and generate the score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating notes...\n",
      "Note 0: F#4\n",
      "Note 1: G#4\n",
      "Note 2: E4\n",
      "Note 3: E4\n",
      "Note 4: F#4\n",
      "Note 5: E4\n",
      "Note 6: F#4\n",
      "Note 7: E4\n",
      "Note 8: G#4\n",
      "Note 9: E4\n",
      "Note 10: F#4\n",
      "Note 11: F#4\n",
      "Note 12: G#4\n",
      "Note 13: E4\n",
      "Note 14: G#4\n",
      "Note 15: E4\n",
      "Note 16: F#4\n",
      "Note 17: E4\n",
      "Note 18: G#4\n",
      "Note 19: E4\n",
      "Note 20: F#4\n",
      "Note 21: F#4\n",
      "Note 22: F#4\n",
      "Note 23: E4\n",
      "Note 24: E4\n",
      "Note 25: G#4\n",
      "Note 26: F#4\n",
      "Note 27: E4\n",
      "Note 28: G#4\n",
      "Note 29: G#4\n",
      "Note 30: F#4\n",
      "Note 31: G#4\n",
      "Note 32: E4\n",
      "Note 33: G#4\n",
      "Note 34: F#4\n",
      "Note 35: G#4\n",
      "Note 36: F#4\n",
      "Note 37: E4\n",
      "Note 38: G#4\n",
      "Note 39: G#4\n",
      "Note 40: E4\n",
      "Note 41: F#4\n",
      "Note 42: F#4\n",
      "Note 43: E4\n",
      "Note 44: G#4\n",
      "Note 45: F#4\n",
      "Note 46: E4\n",
      "Note 47: E4\n",
      "Note 48: F#4\n",
      "Note 49: E4\n",
      "Note 50: F#4\n",
      "Note 51: G#4\n",
      "Note 52: G#4\n",
      "Note 53: G#4\n",
      "Note 54: F#4\n",
      "Note 55: F#4\n",
      "Note 56: F#4\n",
      "Note 57: E4\n",
      "Note 58: F#4\n",
      "Note 59: E4\n",
      "Note 60: F#4\n",
      "Note 61: F#4\n",
      "Note 62: G#4\n",
      "Note 63: G#4\n",
      "Note 64: F#4\n",
      "Note 65: E4\n",
      "Note 66: F#4\n",
      "Note 67: E4\n",
      "Note 68: E4\n",
      "Note 69: G#4\n",
      "Note 70: F#4\n",
      "Note 71: F#4\n",
      "Note 72: G#4\n",
      "Note 73: F#4\n",
      "Note 74: G#4\n",
      "Note 75: F#4\n",
      "Note 76: G#4\n",
      "Note 77: E4\n",
      "Note 78: F#4\n",
      "Note 79: F#4\n",
      "Note 80: G#4\n",
      "Note 81: F#4\n",
      "Note 82: F#4\n",
      "Note 83: F#4\n",
      "Note 84: F#4\n",
      "Note 85: E4\n",
      "Note 86: F#4\n",
      "Note 87: G#4\n",
      "Note 88: G#4\n",
      "Note 89: G#4\n",
      "Note 90: G#4\n",
      "Note 91: G#4\n",
      "Note 92: G#4\n",
      "Note 93: G#4\n",
      "Note 94: E4\n",
      "Note 95: E4\n",
      "Note 96: F#4\n",
      "Note 97: F#4\n",
      "Note 98: F#4\n",
      "Note 99: G#4\n",
      "Note 100: G#4\n",
      "Note 101: E4\n",
      "Note 102: F#4\n",
      "Note 103: G#4\n",
      "Note 104: G#4\n",
      "Note 105: E4\n",
      "Note 106: G#4\n",
      "Note 107: G#4\n",
      "Note 108: F#4\n",
      "Note 109: F#4\n",
      "Note 110: G#4\n",
      "Note 111: E4\n",
      "Note 112: E4\n",
      "Note 113: G#4\n",
      "Note 114: F#4\n",
      "Note 115: F#4\n",
      "Note 116: F#4\n",
      "Note 117: G#4\n",
      "Note 118: F#4\n",
      "Note 119: E4\n",
      "Note 120: F#4\n",
      "Note 121: G#4\n",
      "Note 122: G#4\n",
      "Note 123: E4\n",
      "Note 124: F#4\n",
      "Note 125: F#4\n",
      "Note 126: F#4\n",
      "Note 127: E4\n",
      "Note 128: G#4\n",
      "Note 129: E4\n",
      "Note 130: F#4\n",
      "Note 131: G#4\n",
      "Note 132: E4\n",
      "Note 133: G#4\n",
      "Note 134: F#4\n",
      "Note 135: E4\n",
      "Note 136: F#4\n",
      "Note 137: F#4\n",
      "Note 138: F#4\n",
      "Note 139: E4\n",
      "Note 140: G#4\n",
      "Note 141: E4\n",
      "Note 142: G#4\n",
      "Note 143: F#4\n",
      "Note 144: G#4\n",
      "Note 145: F#4\n",
      "Note 146: G#4\n",
      "Note 147: F#4\n",
      "Note 148: G#4\n",
      "Note 149: E4\n",
      "Note 150: E4\n",
      "Note 151: E4\n",
      "Note 152: G#4\n",
      "Note 153: G#4\n",
      "Note 154: G#4\n",
      "Note 155: E4\n",
      "Note 156: F#4\n",
      "Note 157: F#4\n",
      "Note 158: G#4\n",
      "Note 159: G#4\n",
      "Note 160: F#4\n",
      "Note 161: F#4\n",
      "Note 162: G#4\n",
      "Note 163: G#4\n",
      "Note 164: F#4\n",
      "Note 165: E4\n",
      "Note 166: F#4\n",
      "Note 167: F#4\n",
      "Note 168: F#4\n",
      "Note 169: E4\n",
      "Note 170: E4\n",
      "Note 171: G#4\n",
      "Note 172: E4\n",
      "Note 173: G#4\n",
      "Note 174: F#4\n",
      "Note 175: G#4\n",
      "Note 176: E4\n",
      "Note 177: G#4\n",
      "Note 178: F#4\n",
      "Note 179: E4\n",
      "Note 180: F#4\n",
      "Note 181: F#4\n",
      "Note 182: F#4\n",
      "Note 183: F#4\n",
      "Note 184: G#4\n",
      "Note 185: F#4\n",
      "Note 186: E4\n",
      "Note 187: G#4\n",
      "Note 188: F#4\n",
      "Note 189: F#4\n",
      "Note 190: G#4\n",
      "Note 191: E4\n",
      "Note 192: E4\n",
      "Note 193: F#4\n",
      "Note 194: F#4\n",
      "Note 195: G#4\n",
      "Note 196: F#4\n",
      "Note 197: F#4\n",
      "Note 198: G#4\n",
      "Note 199: E4\n",
      "Note generation complete!\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Generate music based on a starting sequence using the trained neural network.\n",
    "\n",
    "This process uses the model to predict the next notes and constructs\n",
    "a new sequence that can be converted into a MIDI file.\n",
    "\"\"\"\n",
    "\n",
    "import random\n",
    "\n",
    "# Choose a random sequence from network_input as the starting point for generation\n",
    "start = numpy.random.randint(0, len(network_input) - 1)\n",
    "pattern = network_input[start]\n",
    "\n",
    "# Map integers back to their corresponding notes/chords\n",
    "int_to_note = {number: note for number, note in enumerate(pitchnames)}\n",
    "\n",
    "# Store the generated sequence\n",
    "prediction_output = []\n",
    "\n",
    "print(\"Generating notes...\")\n",
    "\n",
    "# Generate a sequence of notes (adjust the range for sequence length)\n",
    "for note_index in range(200):  # Generate more notes for richer output\n",
    "    # Prepare the input for prediction\n",
    "    prediction_input = numpy.reshape(pattern, (1, len(pattern), 1))\n",
    "    prediction_input = prediction_input / float(n_vocab)  # Normalize input\n",
    "\n",
    "    # Predict probabilities for the next note\n",
    "    prediction = model.predict(prediction_input, verbose=0)\n",
    "\n",
    "    # Add some randomness to predictions for creativity\n",
    "    top_indices = numpy.argsort(prediction[0])[-3:]  # Pick the top 3 predictions\n",
    "    index = random.choices(top_indices, weights=prediction[0][top_indices])[0]\n",
    "\n",
    "    # Map the predicted index to the corresponding note\n",
    "    result = int_to_note[index]\n",
    "    prediction_output.append(result)\n",
    "\n",
    "    print(f\"Note {note_index}: {result}\")\n",
    "\n",
    "    # Shift the prediction window and append the new note\n",
    "    pattern.append(index)\n",
    "    pattern = pattern[1:len(pattern)]\n",
    "\n",
    "print(\"Note generation complete!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Convert predicted output to notes and create a MIDI file from the notes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MIDI file successfully created: generated_music.mid\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Convert the predicted output into a MIDI file.\n",
    "\n",
    "This script takes the generated sequence of notes and chords, creates\n",
    "corresponding MIDI objects, and saves them as a MIDI file.\n",
    "\"\"\"\n",
    "\n",
    "from music21 import stream, note, chord, instrument\n",
    "\n",
    "# Initialize variables for MIDI creation\n",
    "offset = 0  # Time spacing between notes/chords\n",
    "output_notes = []\n",
    "\n",
    "# Convert the predicted patterns into notes and chords\n",
    "for pattern in prediction_output:\n",
    "    # If the pattern represents a chord\n",
    "    if ('.' in pattern) or pattern.isdigit():\n",
    "        notes_in_chord = pattern.split('.')\n",
    "        notes = []\n",
    "        for current_note in notes_in_chord:\n",
    "            try:\n",
    "                # Convert the note number into a Note object\n",
    "                new_note = note.Note(int(current_note))\n",
    "                new_note.storedInstrument = instrument.Piano()\n",
    "                notes.append(new_note)\n",
    "            except ValueError:\n",
    "                print(f\"Skipped invalid note: {current_note}\")\n",
    "        # Create a Chord object from the notes\n",
    "        if notes:\n",
    "            new_chord = chord.Chord(notes)\n",
    "            new_chord.offset = offset\n",
    "            output_notes.append(new_chord)\n",
    "    # If the pattern represents a single note\n",
    "    else:\n",
    "        try:\n",
    "            # Convert the pattern into a Note object\n",
    "            new_note = note.Note(pattern)\n",
    "            new_note.offset = offset\n",
    "            new_note.storedInstrument = instrument.Piano()\n",
    "            output_notes.append(new_note)\n",
    "        except Exception as e:\n",
    "            print(f\"Error creating note '{pattern}': {e}\")\n",
    "\n",
    "    # Increment the offset for spacing\n",
    "    offset += 0.5\n",
    "\n",
    "# Create a music21 stream from the generated notes and chords\n",
    "midi_stream = stream.Stream(output_notes)\n",
    "\n",
    "# Save the stream as a MIDI file\n",
    "output_filename = 'generated_music.mid'\n",
    "try:\n",
    "    midi_stream.write('midi', fp=output_filename)\n",
    "    print(f\"MIDI file successfully created: {output_filename}\")\n",
    "except Exception as e:\n",
    "    print(f\"Error writing MIDI file: {e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
