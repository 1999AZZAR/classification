{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fc0b32eb-d2b4-4c4f-861a-932fc5d48a7c",
   "metadata": {},
   "source": [
    "# 0. install lib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ed071e75-8344-42b8-b3c8-b12b59ab6e5f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (1.26.3)\n",
      "Collecting seaborn\n",
      "  Downloading seaborn-0.13.2-py3-none-any.whl.metadata (5.4 kB)\n",
      "Collecting matplotlib\n",
      "  Downloading matplotlib-3.9.2-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (11 kB)\n",
      "Collecting tensorflow[and-cuda]\n",
      "  Downloading tensorflow-2.18.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.1 kB)\n",
      "Collecting absl-py>=1.0.0 (from tensorflow[and-cuda])\n",
      "  Downloading absl_py-2.1.0-py3-none-any.whl.metadata (2.3 kB)\n",
      "Collecting astunparse>=1.6.0 (from tensorflow[and-cuda])\n",
      "  Downloading astunparse-1.6.3-py2.py3-none-any.whl.metadata (4.4 kB)\n",
      "Collecting flatbuffers>=24.3.25 (from tensorflow[and-cuda])\n",
      "  Downloading flatbuffers-24.3.25-py2.py3-none-any.whl.metadata (850 bytes)\n",
      "Collecting gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 (from tensorflow[and-cuda])\n",
      "  Downloading gast-0.6.0-py3-none-any.whl.metadata (1.3 kB)\n",
      "Collecting google-pasta>=0.1.1 (from tensorflow[and-cuda])\n",
      "  Downloading google_pasta-0.2.0-py3-none-any.whl.metadata (814 bytes)\n",
      "Collecting libclang>=13.0.0 (from tensorflow[and-cuda])\n",
      "  Downloading libclang-18.1.1-py2.py3-none-manylinux2010_x86_64.whl.metadata (5.2 kB)\n",
      "Collecting opt-einsum>=2.3.2 (from tensorflow[and-cuda])\n",
      "  Downloading opt_einsum-3.4.0-py3-none-any.whl.metadata (6.3 kB)\n",
      "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from tensorflow[and-cuda]) (24.1)\n",
      "Collecting protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<6.0.0dev,>=3.20.3 (from tensorflow[and-cuda])\n",
      "  Downloading protobuf-5.28.3-cp38-abi3-manylinux2014_x86_64.whl.metadata (592 bytes)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow[and-cuda]) (2.32.3)\n",
      "Requirement already satisfied: setuptools in /usr/local/lib/python3.11/dist-packages (from tensorflow[and-cuda]) (75.1.0)\n",
      "Requirement already satisfied: six>=1.12.0 in /usr/lib/python3/dist-packages (from tensorflow[and-cuda]) (1.16.0)\n",
      "Collecting termcolor>=1.1.0 (from tensorflow[and-cuda])\n",
      "  Downloading termcolor-2.5.0-py3-none-any.whl.metadata (6.1 kB)\n",
      "Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.11/dist-packages (from tensorflow[and-cuda]) (4.9.0)\n",
      "Collecting wrapt>=1.11.0 (from tensorflow[and-cuda])\n",
      "  Downloading wrapt-1.16.0-cp311-cp311-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.6 kB)\n",
      "Collecting grpcio<2.0,>=1.24.3 (from tensorflow[and-cuda])\n",
      "  Downloading grpcio-1.67.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.9 kB)\n",
      "Collecting tensorboard<2.19,>=2.18 (from tensorflow[and-cuda])\n",
      "  Downloading tensorboard-2.18.0-py3-none-any.whl.metadata (1.6 kB)\n",
      "Collecting keras>=3.5.0 (from tensorflow[and-cuda])\n",
      "  Downloading keras-3.6.0-py3-none-any.whl.metadata (5.8 kB)\n",
      "Collecting h5py>=3.11.0 (from tensorflow[and-cuda])\n",
      "  Downloading h5py-3.12.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (2.5 kB)\n",
      "Collecting ml-dtypes<0.5.0,>=0.4.0 (from tensorflow[and-cuda])\n",
      "  Downloading ml_dtypes-0.4.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (20 kB)\n",
      "Collecting tensorflow-io-gcs-filesystem>=0.23.1 (from tensorflow[and-cuda])\n",
      "  Downloading tensorflow_io_gcs_filesystem-0.37.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (14 kB)\n",
      "Collecting nvidia-cublas-cu12==12.5.3.2 (from tensorflow[and-cuda])\n",
      "  Downloading nvidia_cublas_cu12-12.5.3.2-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting nvidia-cuda-cupti-cu12==12.5.82 (from tensorflow[and-cuda])\n",
      "  Downloading nvidia_cuda_cupti_cu12-12.5.82-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
      "Collecting nvidia-cuda-nvcc-cu12==12.5.82 (from tensorflow[and-cuda])\n",
      "  Downloading nvidia_cuda_nvcc_cu12-12.5.82-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting nvidia-cuda-nvrtc-cu12==12.5.82 (from tensorflow[and-cuda])\n",
      "  Downloading nvidia_cuda_nvrtc_cu12-12.5.82-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting nvidia-cuda-runtime-cu12==12.5.82 (from tensorflow[and-cuda])\n",
      "  Downloading nvidia_cuda_runtime_cu12-12.5.82-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting nvidia-cudnn-cu12==9.3.0.75 (from tensorflow[and-cuda])\n",
      "  Downloading nvidia_cudnn_cu12-9.3.0.75-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
      "Collecting nvidia-cufft-cu12==11.2.3.61 (from tensorflow[and-cuda])\n",
      "  Downloading nvidia_cufft_cu12-11.2.3.61-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting nvidia-curand-cu12==10.3.6.82 (from tensorflow[and-cuda])\n",
      "  Downloading nvidia_curand_cu12-10.3.6.82-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting nvidia-cusolver-cu12==11.6.3.83 (from tensorflow[and-cuda])\n",
      "  Downloading nvidia_cusolver_cu12-11.6.3.83-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
      "Collecting nvidia-cusparse-cu12==12.5.1.3 (from tensorflow[and-cuda])\n",
      "  Downloading nvidia_cusparse_cu12-12.5.1.3-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
      "Collecting nvidia-nccl-cu12==2.21.5 (from tensorflow[and-cuda])\n",
      "  Downloading nvidia_nccl_cu12-2.21.5-py3-none-manylinux2014_x86_64.whl.metadata (1.8 kB)\n",
      "Collecting nvidia-nvjitlink-cu12==12.5.82 (from tensorflow[and-cuda])\n",
      "  Downloading nvidia_nvjitlink_cu12-12.5.82-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting pandas>=1.2 (from seaborn)\n",
      "  Downloading pandas-2.2.3-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (89 kB)\n",
      "Collecting contourpy>=1.0.1 (from matplotlib)\n",
      "  Downloading contourpy-1.3.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (5.4 kB)\n",
      "Collecting cycler>=0.10 (from matplotlib)\n",
      "  Downloading cycler-0.12.1-py3-none-any.whl.metadata (3.8 kB)\n",
      "Collecting fonttools>=4.22.0 (from matplotlib)\n",
      "  Downloading fonttools-4.54.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (163 kB)\n",
      "Collecting kiwisolver>=1.3.1 (from matplotlib)\n",
      "  Downloading kiwisolver-1.4.7-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.3 kB)\n",
      "Requirement already satisfied: pillow>=8 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (10.2.0)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in /usr/lib/python3/dist-packages (from matplotlib) (2.4.7)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (2.9.0.post0)\n",
      "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from astunparse>=1.6.0->tensorflow[and-cuda]) (0.44.0)\n",
      "Collecting rich (from keras>=3.5.0->tensorflow[and-cuda])\n",
      "  Downloading rich-13.9.4-py3-none-any.whl.metadata (18 kB)\n",
      "Collecting namex (from keras>=3.5.0->tensorflow[and-cuda])\n",
      "  Downloading namex-0.0.8-py3-none-any.whl.metadata (246 bytes)\n",
      "Collecting optree (from keras>=3.5.0->tensorflow[and-cuda])\n",
      "  Downloading optree-0.13.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (47 kB)\n",
      "Collecting pytz>=2020.1 (from pandas>=1.2->seaborn)\n",
      "  Downloading pytz-2024.2-py2.py3-none-any.whl.metadata (22 kB)\n",
      "Collecting tzdata>=2022.7 (from pandas>=1.2->seaborn)\n",
      "  Downloading tzdata-2024.2-py2.py3-none-any.whl.metadata (1.4 kB)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.21.0->tensorflow[and-cuda]) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.21.0->tensorflow[and-cuda]) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.21.0->tensorflow[and-cuda]) (2.2.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.21.0->tensorflow[and-cuda]) (2024.8.30)\n",
      "Collecting markdown>=2.6.8 (from tensorboard<2.19,>=2.18->tensorflow[and-cuda])\n",
      "  Downloading Markdown-3.7-py3-none-any.whl.metadata (7.0 kB)\n",
      "Collecting tensorboard-data-server<0.8.0,>=0.7.0 (from tensorboard<2.19,>=2.18->tensorflow[and-cuda])\n",
      "  Downloading tensorboard_data_server-0.7.2-py3-none-manylinux_2_31_x86_64.whl.metadata (1.1 kB)\n",
      "Collecting werkzeug>=1.0.1 (from tensorboard<2.19,>=2.18->tensorflow[and-cuda])\n",
      "  Downloading werkzeug-3.1.1-py3-none-any.whl.metadata (3.7 kB)\n",
      "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.11/dist-packages (from werkzeug>=1.0.1->tensorboard<2.19,>=2.18->tensorflow[and-cuda]) (2.1.5)\n",
      "Collecting markdown-it-py>=2.2.0 (from rich->keras>=3.5.0->tensorflow[and-cuda])\n",
      "  Downloading markdown_it_py-3.0.0-py3-none-any.whl.metadata (6.9 kB)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from rich->keras>=3.5.0->tensorflow[and-cuda]) (2.18.0)\n",
      "Collecting mdurl~=0.1 (from markdown-it-py>=2.2.0->rich->keras>=3.5.0->tensorflow[and-cuda])\n",
      "  Downloading mdurl-0.1.2-py3-none-any.whl.metadata (1.6 kB)\n",
      "Downloading nvidia_cublas_cu12-12.5.3.2-py3-none-manylinux2014_x86_64.whl (363.3 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.3/363.3 MB\u001b[0m \u001b[31m57.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cuda_cupti_cu12-12.5.82-py3-none-manylinux2014_x86_64.whl (13.8 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m61.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cuda_nvcc_cu12-12.5.82-py3-none-manylinux2014_x86_64.whl (22.5 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m22.5/22.5 MB\u001b[0m \u001b[31m63.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cuda_nvrtc_cu12-12.5.82-py3-none-manylinux2014_x86_64.whl (24.9 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.9/24.9 MB\u001b[0m \u001b[31m60.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cuda_runtime_cu12-12.5.82-py3-none-manylinux2014_x86_64.whl (895 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m895.7/895.7 kB\u001b[0m \u001b[31m107.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cudnn_cu12-9.3.0.75-py3-none-manylinux2014_x86_64.whl (577.2 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m577.2/577.2 MB\u001b[0m \u001b[31m57.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cufft_cu12-11.2.3.61-py3-none-manylinux2014_x86_64.whl (192.5 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m192.5/192.5 MB\u001b[0m \u001b[31m61.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_curand_cu12-10.3.6.82-py3-none-manylinux2014_x86_64.whl (56.3 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m62.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cusolver_cu12-11.6.3.83-py3-none-manylinux2014_x86_64.whl (130.3 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m130.3/130.3 MB\u001b[0m \u001b[31m60.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cusparse_cu12-12.5.1.3-py3-none-manylinux2014_x86_64.whl (217.6 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m217.6/217.6 MB\u001b[0m \u001b[31m60.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_nccl_cu12-2.21.5-py3-none-manylinux2014_x86_64.whl (188.7 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m188.7/188.7 MB\u001b[0m \u001b[31m60.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_nvjitlink_cu12-12.5.82-py3-none-manylinux2014_x86_64.whl (21.3 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.3/21.3 MB\u001b[0m \u001b[31m62.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading seaborn-0.13.2-py3-none-any.whl (294 kB)\n",
      "Downloading matplotlib-3.9.2-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (8.3 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m8.3/8.3 MB\u001b[0m \u001b[31m66.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading absl_py-2.1.0-py3-none-any.whl (133 kB)\n",
      "Downloading astunparse-1.6.3-py2.py3-none-any.whl (12 kB)\n",
      "Downloading contourpy-1.3.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (323 kB)\n",
      "Downloading cycler-0.12.1-py3-none-any.whl (8.3 kB)\n",
      "Downloading flatbuffers-24.3.25-py2.py3-none-any.whl (26 kB)\n",
      "Downloading fonttools-4.54.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (4.9 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.9/4.9 MB\u001b[0m \u001b[31m69.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading gast-0.6.0-py3-none-any.whl (21 kB)\n",
      "Downloading google_pasta-0.2.0-py3-none-any.whl (57 kB)\n",
      "Downloading grpcio-1.67.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (5.9 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.9/5.9 MB\u001b[0m \u001b[31m76.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading h5py-3.12.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (5.5 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.5/5.5 MB\u001b[0m \u001b[31m75.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading keras-3.6.0-py3-none-any.whl (1.2 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m147.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading kiwisolver-1.4.7-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.4 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.4/1.4 MB\u001b[0m \u001b[31m112.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading libclang-18.1.1-py2.py3-none-manylinux2010_x86_64.whl (24.5 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.5/24.5 MB\u001b[0m \u001b[31m62.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading ml_dtypes-0.4.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.2 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.2/2.2 MB\u001b[0m \u001b[31m118.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading opt_einsum-3.4.0-py3-none-any.whl (71 kB)\n",
      "Downloading pandas-2.2.3-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (13.1 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.1/13.1 MB\u001b[0m \u001b[31m68.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading protobuf-5.28.3-cp38-abi3-manylinux2014_x86_64.whl (316 kB)\n",
      "Downloading tensorboard-2.18.0-py3-none-any.whl (5.5 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.5/5.5 MB\u001b[0m \u001b[31m74.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading tensorflow_io_gcs_filesystem-0.37.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (5.1 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.1/5.1 MB\u001b[0m \u001b[31m71.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading termcolor-2.5.0-py3-none-any.whl (7.8 kB)\n",
      "Downloading wrapt-1.16.0-cp311-cp311-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (80 kB)\n",
      "Downloading tensorflow-2.18.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (615.4 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m615.4/615.4 MB\u001b[0m \u001b[31m54.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading Markdown-3.7-py3-none-any.whl (106 kB)\n",
      "Downloading pytz-2024.2-py2.py3-none-any.whl (508 kB)\n",
      "Downloading tensorboard_data_server-0.7.2-py3-none-manylinux_2_31_x86_64.whl (6.6 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.6/6.6 MB\u001b[0m \u001b[31m80.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading tzdata-2024.2-py2.py3-none-any.whl (346 kB)\n",
      "Downloading werkzeug-3.1.1-py3-none-any.whl (224 kB)\n",
      "Downloading namex-0.0.8-py3-none-any.whl (5.8 kB)\n",
      "Downloading optree-0.13.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (367 kB)\n",
      "Downloading rich-13.9.4-py3-none-any.whl (242 kB)\n",
      "Downloading markdown_it_py-3.0.0-py3-none-any.whl (87 kB)\n",
      "Downloading mdurl-0.1.2-py3-none-any.whl (10.0 kB)\n",
      "Installing collected packages: pytz, namex, libclang, flatbuffers, wrapt, werkzeug, tzdata, termcolor, tensorflow-io-gcs-filesystem, tensorboard-data-server, protobuf, optree, opt-einsum, nvidia-nvjitlink-cu12, nvidia-nccl-cu12, nvidia-curand-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-nvcc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, ml-dtypes, mdurl, markdown, kiwisolver, h5py, grpcio, google-pasta, gast, fonttools, cycler, contourpy, astunparse, absl-py, tensorboard, pandas, nvidia-cusparse-cu12, nvidia-cufft-cu12, nvidia-cudnn-cu12, matplotlib, markdown-it-py, seaborn, rich, nvidia-cusolver-cu12, keras, tensorflow\n",
      "  Attempting uninstall: nvidia-nvjitlink-cu12\n",
      "    Found existing installation: nvidia-nvjitlink-cu12 12.4.99\n",
      "    Uninstalling nvidia-nvjitlink-cu12-12.4.99:\n",
      "      Successfully uninstalled nvidia-nvjitlink-cu12-12.4.99\n",
      "  Attempting uninstall: nvidia-nccl-cu12\n",
      "    Found existing installation: nvidia-nccl-cu12 2.20.5\n",
      "    Uninstalling nvidia-nccl-cu12-2.20.5:\n",
      "      Successfully uninstalled nvidia-nccl-cu12-2.20.5\n",
      "  Attempting uninstall: nvidia-curand-cu12\n",
      "    Found existing installation: nvidia-curand-cu12 10.3.5.119\n",
      "    Uninstalling nvidia-curand-cu12-10.3.5.119:\n",
      "      Successfully uninstalled nvidia-curand-cu12-10.3.5.119\n",
      "  Attempting uninstall: nvidia-cuda-runtime-cu12\n",
      "    Found existing installation: nvidia-cuda-runtime-cu12 12.4.99\n",
      "    Uninstalling nvidia-cuda-runtime-cu12-12.4.99:\n",
      "      Successfully uninstalled nvidia-cuda-runtime-cu12-12.4.99\n",
      "  Attempting uninstall: nvidia-cuda-nvrtc-cu12\n",
      "    Found existing installation: nvidia-cuda-nvrtc-cu12 12.4.99\n",
      "    Uninstalling nvidia-cuda-nvrtc-cu12-12.4.99:\n",
      "      Successfully uninstalled nvidia-cuda-nvrtc-cu12-12.4.99\n",
      "  Attempting uninstall: nvidia-cuda-cupti-cu12\n",
      "    Found existing installation: nvidia-cuda-cupti-cu12 12.4.99\n",
      "    Uninstalling nvidia-cuda-cupti-cu12-12.4.99:\n",
      "      Successfully uninstalled nvidia-cuda-cupti-cu12-12.4.99\n",
      "  Attempting uninstall: nvidia-cublas-cu12\n",
      "    Found existing installation: nvidia-cublas-cu12 12.4.2.65\n",
      "    Uninstalling nvidia-cublas-cu12-12.4.2.65:\n",
      "      Successfully uninstalled nvidia-cublas-cu12-12.4.2.65\n",
      "  Attempting uninstall: nvidia-cusparse-cu12\n",
      "    Found existing installation: nvidia-cusparse-cu12 12.3.0.142\n",
      "    Uninstalling nvidia-cusparse-cu12-12.3.0.142:\n",
      "      Successfully uninstalled nvidia-cusparse-cu12-12.3.0.142\n",
      "  Attempting uninstall: nvidia-cufft-cu12\n",
      "    Found existing installation: nvidia-cufft-cu12 11.2.0.44\n",
      "    Uninstalling nvidia-cufft-cu12-11.2.0.44:\n",
      "      Successfully uninstalled nvidia-cufft-cu12-11.2.0.44\n",
      "  Attempting uninstall: nvidia-cudnn-cu12\n",
      "    Found existing installation: nvidia-cudnn-cu12 9.1.0.70\n",
      "    Uninstalling nvidia-cudnn-cu12-9.1.0.70:\n",
      "      Successfully uninstalled nvidia-cudnn-cu12-9.1.0.70\n",
      "  Attempting uninstall: nvidia-cusolver-cu12\n",
      "    Found existing installation: nvidia-cusolver-cu12 11.6.0.99\n",
      "    Uninstalling nvidia-cusolver-cu12-11.6.0.99:\n",
      "      Successfully uninstalled nvidia-cusolver-cu12-11.6.0.99\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "torch 2.4.1+cu124 requires nvidia-cublas-cu12==12.4.2.65; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cublas-cu12 12.5.3.2 which is incompatible.\n",
      "torch 2.4.1+cu124 requires nvidia-cuda-cupti-cu12==12.4.99; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cuda-cupti-cu12 12.5.82 which is incompatible.\n",
      "torch 2.4.1+cu124 requires nvidia-cuda-nvrtc-cu12==12.4.99; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cuda-nvrtc-cu12 12.5.82 which is incompatible.\n",
      "torch 2.4.1+cu124 requires nvidia-cuda-runtime-cu12==12.4.99; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cuda-runtime-cu12 12.5.82 which is incompatible.\n",
      "torch 2.4.1+cu124 requires nvidia-cudnn-cu12==9.1.0.70; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cudnn-cu12 9.3.0.75 which is incompatible.\n",
      "torch 2.4.1+cu124 requires nvidia-cufft-cu12==11.2.0.44; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cufft-cu12 11.2.3.61 which is incompatible.\n",
      "torch 2.4.1+cu124 requires nvidia-curand-cu12==10.3.5.119; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-curand-cu12 10.3.6.82 which is incompatible.\n",
      "torch 2.4.1+cu124 requires nvidia-cusolver-cu12==11.6.0.99; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cusolver-cu12 11.6.3.83 which is incompatible.\n",
      "torch 2.4.1+cu124 requires nvidia-cusparse-cu12==12.3.0.142; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cusparse-cu12 12.5.1.3 which is incompatible.\n",
      "torch 2.4.1+cu124 requires nvidia-nccl-cu12==2.20.5; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-nccl-cu12 2.21.5 which is incompatible.\n",
      "torch 2.4.1+cu124 requires nvidia-nvjitlink-cu12==12.4.99; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-nvjitlink-cu12 12.5.82 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0mSuccessfully installed absl-py-2.1.0 astunparse-1.6.3 contourpy-1.3.0 cycler-0.12.1 flatbuffers-24.3.25 fonttools-4.54.1 gast-0.6.0 google-pasta-0.2.0 grpcio-1.67.1 h5py-3.12.1 keras-3.6.0 kiwisolver-1.4.7 libclang-18.1.1 markdown-3.7 markdown-it-py-3.0.0 matplotlib-3.9.2 mdurl-0.1.2 ml-dtypes-0.4.1 namex-0.0.8 nvidia-cublas-cu12-12.5.3.2 nvidia-cuda-cupti-cu12-12.5.82 nvidia-cuda-nvcc-cu12-12.5.82 nvidia-cuda-nvrtc-cu12-12.5.82 nvidia-cuda-runtime-cu12-12.5.82 nvidia-cudnn-cu12-9.3.0.75 nvidia-cufft-cu12-11.2.3.61 nvidia-curand-cu12-10.3.6.82 nvidia-cusolver-cu12-11.6.3.83 nvidia-cusparse-cu12-12.5.1.3 nvidia-nccl-cu12-2.21.5 nvidia-nvjitlink-cu12-12.5.82 opt-einsum-3.4.0 optree-0.13.0 pandas-2.2.3 protobuf-5.28.3 pytz-2024.2 rich-13.9.4 seaborn-0.13.2 tensorboard-2.18.0 tensorboard-data-server-0.7.2 tensorflow-2.18.0 tensorflow-io-gcs-filesystem-0.37.1 termcolor-2.5.0 tzdata-2024.2 werkzeug-3.1.1 wrapt-1.16.0\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager, possibly rendering your system unusable.It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv. Use the --root-user-action option if you know what you are doing and want to suppress this warning.\u001b[0m\u001b[33m\n",
      "\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.3.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython -m pip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install numpy tensorflow[and-cuda] seaborn matplotlib"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6dd6e1c-0599-4167-842c-aa3da1adcda4",
   "metadata": {},
   "source": [
    "# 1. Load and Preprocess Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84da5913-4261-470c-8b56-165851a648f1",
   "metadata": {},
   "source": [
    "## import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "73363e1c-12ac-4677-81f7-8f5af220c63c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, Model, callbacks\n",
    "from tensorflow.keras.datasets import mnist\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b069d474-8de4-466c-abe3-38ae5777f101",
   "metadata": {},
   "source": [
    "## Set random seeds for reproducibility"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cb3e85b7-9553-4e09-b54e-7c6bce74aed3",
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.random.set_seed(1337)\n",
    "np.random.seed(1337)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29b06c17-46f6-412d-b1e6-e6492d428f6a",
   "metadata": {},
   "source": [
    "## Load and preprocess the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ac664499-c8b1-4bab-b8d4-03f927180e90",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading and preprocessing data...\n",
      "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/mnist.npz\n",
      "\u001b[1m11490434/11490434\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 0us/step\n",
      "Data loaded and preprocessed.\n"
     ]
    }
   ],
   "source": [
    "print(\"Loading and preprocessing data...\")\n",
    "(x_train, _), (x_test, _) = mnist.load_data()\n",
    "x_train = x_train.astype('float32') / 255.0\n",
    "x_test = x_test.astype('float32') / 255.0\n",
    "x_train = np.reshape(x_train, (len(x_train), 28, 28, 1))\n",
    "x_test = np.reshape(x_test, (len(x_test), 28, 28, 1))\n",
    "print(\"Data loaded and preprocessed.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0472c9b1-f0b0-4078-b0c6-84394ec17aa2",
   "metadata": {},
   "source": [
    "# 2. Build Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7e2bf043-dbb3-4f7a-a29a-cdd13b925e6a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating the encoder...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0000 00:00:1730535511.826703     291 gpu_device.cc:2022] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 18446 MB memory:  -> device: 0, name: NVIDIA RTX 4000 Ada Generation, pci bus id: 0000:81:00.0, compute capability: 8.9\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoder created.\n"
     ]
    }
   ],
   "source": [
    "# Create the encoder model\n",
    "print(\"Creating the encoder...\")\n",
    "input_img = layers.Input(shape=(28, 28, 1))\n",
    "x = layers.Flatten()(input_img)\n",
    "x = layers.Dense(1000, activation='relu')(x)\n",
    "x = layers.Dense(500, activation='relu')(x)\n",
    "x = layers.Dense(250, activation='relu')(x)\n",
    "encoded_output = layers.Dense(2, activation='relu')(x)\n",
    "encoder = Model(input_img, encoded_output)\n",
    "print(\"Encoder created.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2557f160-8fdf-4081-80e9-1da30a3be107",
   "metadata": {},
   "source": [
    "# 3. Build Decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8ccdf5de-c6bf-468a-b868-ff82e1d71e99",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating the decoder...\n",
      "Decoder created.\n"
     ]
    }
   ],
   "source": [
    "# Create the decoder model\n",
    "print(\"Creating the decoder...\")\n",
    "decoder_input = layers.Input(shape=(2,))\n",
    "x = layers.Dense(250, activation='relu')(decoder_input)\n",
    "x = layers.Dense(500, activation='relu')(x)\n",
    "x = layers.Dense(1000, activation='relu')(x)\n",
    "x = layers.Dense(28 * 28, activation='sigmoid')(x)\n",
    "decoded_output = layers.Reshape((28, 28, 1))(x)\n",
    "decoder = Model(decoder_input, decoded_output)\n",
    "print(\"Decoder created.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb1ff2ec-b614-439c-9551-008ffda09281",
   "metadata": {},
   "source": [
    "# 4. Combine Encoder and Decoder into Autoencoder Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4f722527-611c-4488-9a13-b53064a5b01e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Combining encoder and decoder...\n",
      "Autoencoder model created.\n"
     ]
    }
   ],
   "source": [
    "# Combine encoder and decoder\n",
    "print(\"Combining encoder and decoder...\")\n",
    "autoencoder_input = encoder.input  # Use the encoder's input as the input for the autoencoder\n",
    "autoencoder_output = decoder(encoder(autoencoder_input))  # Pass the encoder's output to the decoder\n",
    "autoencoder = Model(inputs=autoencoder_input, outputs=autoencoder_output)\n",
    "print(\"Autoencoder model created.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e5976b8-8aef-4c69-83f7-11e76c303638",
   "metadata": {},
   "source": [
    "# 5. Compile the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4e5d0ce2-f668-4091-b578-246930540a12",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Compiling model...\n",
      "Model compiled.\n"
     ]
    }
   ],
   "source": [
    "# Compile the model\n",
    "print(\"Compiling model...\")\n",
    "autoencoder.compile(optimizer='adam', loss='mse')\n",
    "print(\"Model compiled.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fd45929-8426-4c9b-8522-368984bfdefa",
   "metadata": {},
   "source": [
    "# 6. Train the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e6c8c7f0-27ca-459d-9689-a1690f91cf66",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training model...\n",
      "Epoch 1/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "I0000 00:00:1730535527.446518     643 service.cc:148] XLA service 0x7dd284010550 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n",
      "I0000 00:00:1730535527.446565     643 service.cc:156]   StreamExecutor device (0): NVIDIA RTX 4000 Ada Generation, Compute Capability 8.9\n",
      "2024-11-02 08:18:47.540250: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:268] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.\n",
      "I0000 00:00:1730535527.745019     643 cuda_dnn.cc:529] Loaded cuDNN version 90300\n",
      "2024-11-02 08:18:52.329704: I external/local_xla/xla/stream_executor/cuda/cuda_asm_compiler.cc:397] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_706', 4 bytes spill stores, 4 bytes spill loads\n",
      "\n",
      "2024-11-02 08:18:52.936947: I external/local_xla/xla/stream_executor/cuda/cuda_asm_compiler.cc:397] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_342_0', 4 bytes spill stores, 4 bytes spill loads\n",
      "\n",
      "2024-11-02 08:18:52.944481: I external/local_xla/xla/stream_executor/cuda/cuda_asm_compiler.cc:397] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_293', 60 bytes spill stores, 60 bytes spill loads\n",
      "\n",
      "2024-11-02 08:18:53.338659: I external/local_xla/xla/stream_executor/cuda/cuda_asm_compiler.cc:397] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_328', 84 bytes spill stores, 84 bytes spill loads\n",
      "\n",
      "2024-11-02 08:18:53.427970: I external/local_xla/xla/stream_executor/cuda/cuda_asm_compiler.cc:397] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_293', 64 bytes spill stores, 64 bytes spill loads\n",
      "\n",
      "2024-11-02 08:18:53.528592: I external/local_xla/xla/stream_executor/cuda/cuda_asm_compiler.cc:397] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_293', 256 bytes spill stores, 256 bytes spill loads\n",
      "\n",
      "2024-11-02 08:18:53.530235: I external/local_xla/xla/stream_executor/cuda/cuda_asm_compiler.cc:397] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_293', 280 bytes spill stores, 284 bytes spill loads\n",
      "\n",
      "2024-11-02 08:18:53.839125: I external/local_xla/xla/stream_executor/cuda/cuda_asm_compiler.cc:397] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_349', 276 bytes spill stores, 280 bytes spill loads\n",
      "\n",
      "2024-11-02 08:18:53.936593: I external/local_xla/xla/stream_executor/cuda/cuda_asm_compiler.cc:397] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_314', 256 bytes spill stores, 260 bytes spill loads\n",
      "\n",
      "2024-11-02 08:18:54.227027: I external/local_xla/xla/stream_executor/cuda/cuda_asm_compiler.cc:397] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_293', 236 bytes spill stores, 236 bytes spill loads\n",
      "\n",
      "2024-11-02 08:18:55.226789: I external/local_xla/xla/stream_executor/cuda/cuda_asm_compiler.cc:397] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_293_0', 784 bytes spill stores, 736 bytes spill loads\n",
      "\n",
      "2024-11-02 08:18:55.625622: I external/local_xla/xla/stream_executor/cuda/cuda_asm_compiler.cc:397] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_314_0', 564 bytes spill stores, 528 bytes spill loads\n",
      "\n",
      "2024-11-02 08:18:55.832894: I external/local_xla/xla/stream_executor/cuda/cuda_asm_compiler.cc:397] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_349', 260 bytes spill stores, 260 bytes spill loads\n",
      "\n",
      "2024-11-02 08:18:55.925597: I external/local_xla/xla/stream_executor/cuda/cuda_asm_compiler.cc:397] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_314', 20 bytes spill stores, 20 bytes spill loads\n",
      "\n",
      "2024-11-02 08:18:56.227154: I external/local_xla/xla/stream_executor/cuda/cuda_asm_compiler.cc:397] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_349_0', 776 bytes spill stores, 724 bytes spill loads\n",
      "\n",
      "2024-11-02 08:18:57.445074: I external/local_xla/xla/stream_executor/cuda/cuda_asm_compiler.cc:397] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_342', 32 bytes spill stores, 32 bytes spill loads\n",
      "\n",
      "2024-11-02 08:18:57.837779: I external/local_xla/xla/stream_executor/cuda/cuda_asm_compiler.cc:397] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_314', 64 bytes spill stores, 64 bytes spill loads\n",
      "\n",
      "2024-11-02 08:18:58.736636: I external/local_xla/xla/stream_executor/cuda/cuda_asm_compiler.cc:397] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_356', 236 bytes spill stores, 236 bytes spill loads\n",
      "\n",
      "2024-11-02 08:18:58.832975: I external/local_xla/xla/stream_executor/cuda/cuda_asm_compiler.cc:397] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_342', 68 bytes spill stores, 68 bytes spill loads\n",
      "\n",
      "2024-11-02 08:18:59.132522: I external/local_xla/xla/stream_executor/cuda/cuda_asm_compiler.cc:397] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_349', 24 bytes spill stores, 28 bytes spill loads\n",
      "\n",
      "2024-11-02 08:18:59.144207: I external/local_xla/xla/stream_executor/cuda/cuda_asm_compiler.cc:397] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_342', 564 bytes spill stores, 568 bytes spill loads\n",
      "\n",
      "2024-11-02 08:19:01.630134: I external/local_xla/xla/stream_executor/cuda/cuda_asm_compiler.cc:397] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_342', 24 bytes spill stores, 24 bytes spill loads\n",
      "\n",
      "2024-11-02 08:19:02.139185: I external/local_xla/xla/stream_executor/cuda/cuda_asm_compiler.cc:397] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_356', 256 bytes spill stores, 256 bytes spill loads\n",
      "\n",
      "2024-11-02 08:19:02.248204: I external/local_xla/xla/stream_executor/cuda/cuda_asm_compiler.cc:397] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_718', 4 bytes spill stores, 4 bytes spill loads\n",
      "\n",
      "2024-11-02 08:19:02.838305: I external/local_xla/xla/stream_executor/cuda/cuda_asm_compiler.cc:397] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_356_0', 784 bytes spill stores, 736 bytes spill loads\n",
      "\n",
      "2024-11-02 08:19:03.142944: I external/local_xla/xla/stream_executor/cuda/cuda_asm_compiler.cc:397] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_342', 100 bytes spill stores, 108 bytes spill loads\n",
      "\n",
      "2024-11-02 08:19:03.725570: I external/local_xla/xla/stream_executor/cuda/cuda_asm_compiler.cc:397] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_349', 32 bytes spill stores, 32 bytes spill loads\n",
      "\n",
      "2024-11-02 08:19:04.934854: I external/local_xla/xla/stream_executor/cuda/cuda_asm_compiler.cc:397] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_356', 60 bytes spill stores, 60 bytes spill loads\n",
      "\n",
      "2024-11-02 08:19:05.133110: I external/local_xla/xla/stream_executor/cuda/cuda_asm_compiler.cc:397] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_356', 64 bytes spill stores, 64 bytes spill loads\n",
      "\n",
      "2024-11-02 08:19:06.126570: I external/local_xla/xla/stream_executor/cuda/cuda_asm_compiler.cc:397] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_356', 280 bytes spill stores, 284 bytes spill loads\n",
      "\n",
      "2024-11-02 08:19:07.351143: I external/local_xla/xla/stream_executor/cuda/cuda_asm_compiler.cc:397] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_714_0', 84 bytes spill stores, 84 bytes spill loads\n",
      "\n",
      "2024-11-02 08:19:07.744420: I external/local_xla/xla/stream_executor/cuda/cuda_asm_compiler.cc:397] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_716', 4 bytes spill stores, 4 bytes spill loads\n",
      "\n",
      "2024-11-02 08:19:07.745611: I external/local_xla/xla/stream_executor/cuda/cuda_asm_compiler.cc:397] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_718_0', 84 bytes spill stores, 84 bytes spill loads\n",
      "\n",
      "2024-11-02 08:19:07.956628: I external/local_xla/xla/stream_executor/cuda/cuda_asm_compiler.cc:397] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_708_0', 224 bytes spill stores, 224 bytes spill loads\n",
      "\n",
      "2024-11-02 08:19:08.096802: I external/local_xla/xla/stream_executor/cuda/cuda_asm_compiler.cc:397] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_342', 44 bytes spill stores, 44 bytes spill loads\n",
      "\n",
      "2024-11-02 08:19:08.201847: I external/local_xla/xla/stream_executor/cuda/cuda_asm_compiler.cc:397] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_342', 168 bytes spill stores, 168 bytes spill loads\n",
      "\n",
      "2024-11-02 08:19:08.245826: I external/local_xla/xla/stream_executor/cuda/cuda_asm_compiler.cc:397] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_706_0', 84 bytes spill stores, 84 bytes spill loads\n",
      "\n",
      "2024-11-02 08:19:08.396590: I external/local_xla/xla/stream_executor/cuda/cuda_asm_compiler.cc:397] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_716_0', 84 bytes spill stores, 84 bytes spill loads\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m 83/235\u001b[0m \u001b[32m━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.1116"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0000 00:00:1730535551.120945     643 device_compiler.h:188] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m216/235\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0870"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-02 08:19:16.346709: I external/local_xla/xla/stream_executor/cuda/cuda_asm_compiler.cc:397] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_314', 64 bytes spill stores, 64 bytes spill loads\n",
      "\n",
      "2024-11-02 08:19:17.029601: I external/local_xla/xla/stream_executor/cuda/cuda_asm_compiler.cc:397] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_293', 68 bytes spill stores, 68 bytes spill loads\n",
      "\n",
      "2024-11-02 08:19:17.427992: I external/local_xla/xla/stream_executor/cuda/cuda_asm_compiler.cc:397] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_342', 188 bytes spill stores, 188 bytes spill loads\n",
      "\n",
      "2024-11-02 08:19:17.437103: I external/local_xla/xla/stream_executor/cuda/cuda_asm_compiler.cc:397] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_314', 24 bytes spill stores, 24 bytes spill loads\n",
      "\n",
      "2024-11-02 08:19:17.542201: I external/local_xla/xla/stream_executor/cuda/cuda_asm_compiler.cc:397] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_293', 4 bytes spill stores, 8 bytes spill loads\n",
      "\n",
      "2024-11-02 08:19:17.835972: I external/local_xla/xla/stream_executor/cuda/cuda_asm_compiler.cc:397] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_293', 236 bytes spill stores, 236 bytes spill loads\n",
      "\n",
      "2024-11-02 08:19:18.133158: I external/local_xla/xla/stream_executor/cuda/cuda_asm_compiler.cc:397] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_293', 80 bytes spill stores, 80 bytes spill loads\n",
      "\n",
      "2024-11-02 08:19:18.136817: I external/local_xla/xla/stream_executor/cuda/cuda_asm_compiler.cc:397] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_293', 28 bytes spill stores, 28 bytes spill loads\n",
      "\n",
      "2024-11-02 08:19:18.337475: I external/local_xla/xla/stream_executor/cuda/cuda_asm_compiler.cc:397] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_293_0', 1060 bytes spill stores, 748 bytes spill loads\n",
      "\n",
      "2024-11-02 08:19:20.637948: I external/local_xla/xla/stream_executor/cuda/cuda_asm_compiler.cc:397] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_356', 80 bytes spill stores, 80 bytes spill loads\n",
      "\n",
      "2024-11-02 08:19:20.840476: I external/local_xla/xla/stream_executor/cuda/cuda_asm_compiler.cc:397] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_342', 80 bytes spill stores, 80 bytes spill loads\n",
      "\n",
      "2024-11-02 08:19:21.125835: I external/local_xla/xla/stream_executor/cuda/cuda_asm_compiler.cc:397] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_293', 932 bytes spill stores, 772 bytes spill loads\n",
      "\n",
      "2024-11-02 08:19:21.536741: I external/local_xla/xla/stream_executor/cuda/cuda_asm_compiler.cc:397] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_356', 4 bytes spill stores, 8 bytes spill loads\n",
      "\n",
      "2024-11-02 08:19:21.937304: I external/local_xla/xla/stream_executor/cuda/cuda_asm_compiler.cc:397] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_314', 20 bytes spill stores, 20 bytes spill loads\n",
      "\n",
      "2024-11-02 08:19:22.426641: I external/local_xla/xla/stream_executor/cuda/cuda_asm_compiler.cc:397] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_706', 8 bytes spill stores, 8 bytes spill loads\n",
      "\n",
      "2024-11-02 08:19:22.535838: I external/local_xla/xla/stream_executor/cuda/cuda_asm_compiler.cc:397] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_314', 84 bytes spill stores, 84 bytes spill loads\n",
      "\n",
      "2024-11-02 08:19:22.732490: I external/local_xla/xla/stream_executor/cuda/cuda_asm_compiler.cc:397] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_342', 112 bytes spill stores, 72 bytes spill loads\n",
      "\n",
      "2024-11-02 08:19:22.829761: I external/local_xla/xla/stream_executor/cuda/cuda_asm_compiler.cc:397] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_356_0', 264 bytes spill stores, 264 bytes spill loads\n",
      "\n",
      "2024-11-02 08:19:22.938220: I external/local_xla/xla/stream_executor/cuda/cuda_asm_compiler.cc:397] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_706', 4 bytes spill stores, 4 bytes spill loads\n",
      "\n",
      "2024-11-02 08:19:23.328831: I external/local_xla/xla/stream_executor/cuda/cuda_asm_compiler.cc:397] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_342_0', 44 bytes spill stores, 44 bytes spill loads\n",
      "\n",
      "2024-11-02 08:19:23.336889: I external/local_xla/xla/stream_executor/cuda/cuda_asm_compiler.cc:397] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_342', 744 bytes spill stores, 548 bytes spill loads\n",
      "\n",
      "2024-11-02 08:19:23.442543: I external/local_xla/xla/stream_executor/cuda/cuda_asm_compiler.cc:397] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_356_0', 1060 bytes spill stores, 748 bytes spill loads\n",
      "\n",
      "2024-11-02 08:19:24.137922: I external/local_xla/xla/stream_executor/cuda/cuda_asm_compiler.cc:397] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_314', 1412 bytes spill stores, 1300 bytes spill loads\n",
      "\n",
      "2024-11-02 08:19:25.235518: I external/local_xla/xla/stream_executor/cuda/cuda_asm_compiler.cc:397] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_314_0', 752 bytes spill stores, 576 bytes spill loads\n",
      "\n",
      "2024-11-02 08:19:26.032860: I external/local_xla/xla/stream_executor/cuda/cuda_asm_compiler.cc:397] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_349', 932 bytes spill stores, 764 bytes spill loads\n",
      "\n",
      "2024-11-02 08:19:26.230742: I external/local_xla/xla/stream_executor/cuda/cuda_asm_compiler.cc:397] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_328', 120 bytes spill stores, 120 bytes spill loads\n",
      "\n",
      "2024-11-02 08:19:26.327470: I external/local_xla/xla/stream_executor/cuda/cuda_asm_compiler.cc:397] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_349', 60 bytes spill stores, 60 bytes spill loads\n",
      "\n",
      "2024-11-02 08:19:26.448255: I external/local_xla/xla/stream_executor/cuda/cuda_asm_compiler.cc:397] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_349_0', 1044 bytes spill stores, 732 bytes spill loads\n",
      "\n",
      "2024-11-02 08:19:26.643889: I external/local_xla/xla/stream_executor/cuda/cuda_asm_compiler.cc:397] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_718', 4 bytes spill stores, 4 bytes spill loads\n",
      "\n",
      "2024-11-02 08:19:27.432494: I external/local_xla/xla/stream_executor/cuda/cuda_asm_compiler.cc:397] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_716', 4 bytes spill stores, 4 bytes spill loads\n",
      "\n",
      "2024-11-02 08:19:29.033411: I external/local_xla/xla/stream_executor/cuda/cuda_asm_compiler.cc:397] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_356', 236 bytes spill stores, 236 bytes spill loads\n",
      "\n",
      "2024-11-02 08:19:29.229992: I external/local_xla/xla/stream_executor/cuda/cuda_asm_compiler.cc:397] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_356', 68 bytes spill stores, 68 bytes spill loads\n",
      "\n",
      "2024-11-02 08:19:30.226186: I external/local_xla/xla/stream_executor/cuda/cuda_asm_compiler.cc:397] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_716', 8 bytes spill stores, 8 bytes spill loads\n",
      "\n",
      "2024-11-02 08:19:31.026656: I external/local_xla/xla/stream_executor/cuda/cuda_asm_compiler.cc:397] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_714', 168 bytes spill stores, 168 bytes spill loads\n",
      "\n",
      "2024-11-02 08:19:31.139584: I external/local_xla/xla/stream_executor/cuda/cuda_asm_compiler.cc:397] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_342', 48 bytes spill stores, 48 bytes spill loads\n",
      "\n",
      "2024-11-02 08:19:31.328007: I external/local_xla/xla/stream_executor/cuda/cuda_asm_compiler.cc:397] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_349', 52 bytes spill stores, 52 bytes spill loads\n",
      "\n",
      "2024-11-02 08:19:33.046030: I external/local_xla/xla/stream_executor/cuda/cuda_asm_compiler.cc:397] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_708', 188 bytes spill stores, 188 bytes spill loads\n",
      "\n",
      "2024-11-02 08:19:33.178729: I external/local_xla/xla/stream_executor/cuda/cuda_asm_compiler.cc:397] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_342', 52 bytes spill stores, 56 bytes spill loads\n",
      "\n",
      "2024-11-02 08:19:33.254108: I external/local_xla/xla/stream_executor/cuda/cuda_asm_compiler.cc:397] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_708', 28 bytes spill stores, 28 bytes spill loads\n",
      "\n",
      "2024-11-02 08:19:33.267742: I external/local_xla/xla/stream_executor/cuda/cuda_asm_compiler.cc:397] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_718', 8 bytes spill stores, 8 bytes spill loads\n",
      "\n",
      "2024-11-02 08:19:33.349879: I external/local_xla/xla/stream_executor/cuda/cuda_asm_compiler.cc:397] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_356', 28 bytes spill stores, 28 bytes spill loads\n",
      "\n",
      "2024-11-02 08:19:33.826250: I external/local_xla/xla/stream_executor/cuda/cuda_asm_compiler.cc:397] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_356', 932 bytes spill stores, 772 bytes spill loads\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m235/235\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 112ms/step - loss: 0.0852"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-02 08:19:44.040444: I external/local_xla/xla/stream_executor/cuda/cuda_asm_compiler.cc:397] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_33', 220 bytes spill stores, 228 bytes spill loads\n",
      "\n",
      "2024-11-02 08:19:45.096947: I external/local_xla/xla/stream_executor/cuda/cuda_asm_compiler.cc:397] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_75', 220 bytes spill stores, 228 bytes spill loads\n",
      "\n",
      "2024-11-02 08:19:45.298033: I external/local_xla/xla/stream_executor/cuda/cuda_asm_compiler.cc:397] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_68', 208 bytes spill stores, 208 bytes spill loads\n",
      "\n",
      "2024-11-02 08:19:45.340809: I external/local_xla/xla/stream_executor/cuda/cuda_asm_compiler.cc:397] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_61', 180 bytes spill stores, 180 bytes spill loads\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m235/235\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m61s\u001b[0m 152ms/step - loss: 0.0851 - val_loss: 0.0510\n",
      "Epoch 2/50\n",
      "\u001b[1m235/235\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0490 - val_loss: 0.0447\n",
      "Epoch 3/50\n",
      "\u001b[1m235/235\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0440 - val_loss: 0.0421\n",
      "Epoch 4/50\n",
      "\u001b[1m235/235\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0417 - val_loss: 0.0404\n",
      "Epoch 5/50\n",
      "\u001b[1m235/235\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0405 - val_loss: 0.0396\n",
      "Epoch 6/50\n",
      "\u001b[1m235/235\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0396 - val_loss: 0.0389\n",
      "Epoch 7/50\n",
      "\u001b[1m235/235\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0390 - val_loss: 0.0383\n",
      "Epoch 8/50\n",
      "\u001b[1m235/235\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0381 - val_loss: 0.0379\n",
      "Epoch 9/50\n",
      "\u001b[1m235/235\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0378 - val_loss: 0.0378\n",
      "Epoch 10/50\n",
      "\u001b[1m235/235\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0376 - val_loss: 0.0372\n",
      "Epoch 11/50\n",
      "\u001b[1m235/235\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0370 - val_loss: 0.0369\n",
      "Epoch 12/50\n",
      "\u001b[1m235/235\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0367 - val_loss: 0.0367\n",
      "Epoch 13/50\n",
      "\u001b[1m235/235\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0363 - val_loss: 0.0367\n",
      "Epoch 14/50\n",
      "\u001b[1m235/235\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0362 - val_loss: 0.0361\n",
      "Epoch 15/50\n",
      "\u001b[1m235/235\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0359 - val_loss: 0.0360\n",
      "Epoch 16/50\n",
      "\u001b[1m235/235\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0358 - val_loss: 0.0357\n",
      "Epoch 17/50\n",
      "\u001b[1m235/235\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0354 - val_loss: 0.0357\n",
      "Epoch 18/50\n",
      "\u001b[1m235/235\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0353 - val_loss: 0.0360\n",
      "Epoch 19/50\n",
      "\u001b[1m235/235\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0356 - val_loss: 0.0353\n",
      "Epoch 20/50\n",
      "\u001b[1m235/235\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0349 - val_loss: 0.0351\n",
      "Epoch 21/50\n",
      "\u001b[1m235/235\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0348 - val_loss: 0.0351\n",
      "Epoch 22/50\n",
      "\u001b[1m235/235\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0347 - val_loss: 0.0350\n",
      "Epoch 23/50\n",
      "\u001b[1m235/235\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0345 - val_loss: 0.0349\n",
      "Epoch 24/50\n",
      "\u001b[1m235/235\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0343 - val_loss: 0.0350\n",
      "Epoch 25/50\n",
      "\u001b[1m235/235\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0344 - val_loss: 0.0348\n",
      "Epoch 26/50\n",
      "\u001b[1m235/235\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0343 - val_loss: 0.0346\n",
      "Epoch 27/50\n",
      "\u001b[1m235/235\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0340 - val_loss: 0.0347\n",
      "Epoch 28/50\n",
      "\u001b[1m235/235\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.0341 - val_loss: 0.0346\n",
      "Epoch 29/50\n",
      "\u001b[1m235/235\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.0339 - val_loss: 0.0343\n",
      "Epoch 30/50\n",
      "\u001b[1m235/235\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.0337 - val_loss: 0.0346\n",
      "Epoch 31/50\n",
      "\u001b[1m235/235\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0337 - val_loss: 0.0343\n",
      "Epoch 32/50\n",
      "\u001b[1m235/235\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0336 - val_loss: 0.0344\n",
      "Epoch 33/50\n",
      "\u001b[1m235/235\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0335 - val_loss: 0.0342\n",
      "Epoch 34/50\n",
      "\u001b[1m235/235\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0334 - val_loss: 0.0342\n",
      "Epoch 35/50\n",
      "\u001b[1m235/235\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0333 - val_loss: 0.0340\n",
      "Epoch 36/50\n",
      "\u001b[1m235/235\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0331 - val_loss: 0.0341\n",
      "Epoch 37/50\n",
      "\u001b[1m235/235\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0332 - val_loss: 0.0341\n",
      "Epoch 38/50\n",
      "\u001b[1m235/235\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0331 - val_loss: 0.0342\n",
      "Model training complete.\n"
     ]
    }
   ],
   "source": [
    "# Train the model\n",
    "print(\"Training model...\")\n",
    "early_stop = callbacks.EarlyStopping(monitor='val_loss', patience=3)\n",
    "history = autoencoder.fit(\n",
    "    x_train, x_train,\n",
    "    epochs=50,\n",
    "    batch_size=256,\n",
    "    shuffle=True,\n",
    "    validation_data=(x_test, x_test),\n",
    "    callbacks=[early_stop]\n",
    ")\n",
    "print(\"Model training complete.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbaeb21e-6074-42d6-a069-3f008809718c",
   "metadata": {},
   "source": [
    "# 7. Visualize the Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "225114e0-e7f6-42f2-bbf3-acfba02685e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Visualizing results...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-02 08:20:59.936725: I external/local_xla/xla/stream_executor/cuda/cuda_asm_compiler.cc:397] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_34', 64 bytes spill stores, 64 bytes spill loads\n",
      "\n",
      "2024-11-02 08:21:00.040595: I external/local_xla/xla/stream_executor/cuda/cuda_asm_compiler.cc:397] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_27', 408 bytes spill stores, 444 bytes spill loads\n",
      "\n",
      "2024-11-02 08:21:01.630652: I external/local_xla/xla/stream_executor/cuda/cuda_asm_compiler.cc:397] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_55', 372 bytes spill stores, 404 bytes spill loads\n",
      "\n",
      "2024-11-02 08:21:02.149897: I external/local_xla/xla/stream_executor/cuda/cuda_asm_compiler.cc:397] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_34', 116 bytes spill stores, 116 bytes spill loads\n",
      "\n",
      "2024-11-02 08:21:02.934262: I external/local_xla/xla/stream_executor/cuda/cuda_asm_compiler.cc:397] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_69', 236 bytes spill stores, 236 bytes spill loads\n",
      "\n",
      "2024-11-02 08:21:03.229023: I external/local_xla/xla/stream_executor/cuda/cuda_asm_compiler.cc:397] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_69', 408 bytes spill stores, 444 bytes spill loads\n",
      "\n",
      "2024-11-02 08:21:03.434822: I external/local_xla/xla/stream_executor/cuda/cuda_asm_compiler.cc:397] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_27', 236 bytes spill stores, 236 bytes spill loads\n",
      "\n",
      "2024-11-02 08:21:03.635650: I external/local_xla/xla/stream_executor/cuda/cuda_asm_compiler.cc:397] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_62', 396 bytes spill stores, 432 bytes spill loads\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 6ms/step\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 2000x400 with 20 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Visualization complete.\n"
     ]
    }
   ],
   "source": [
    "# Visualize results\n",
    "print(\"Visualizing results...\")\n",
    "decoded_imgs = autoencoder.predict(x_test)\n",
    "n = 10\n",
    "plt.figure(figsize=(20, 4))\n",
    "for i in range(n):\n",
    "    # Display original\n",
    "    ax = plt.subplot(2, n, i + 1)\n",
    "    plt.imshow(x_test[i].reshape(28, 28), cmap='gray')\n",
    "    plt.axis('off')\n",
    "\n",
    "    # Display reconstruction\n",
    "    ax = plt.subplot(2, n, i + 1 + n)\n",
    "    plt.imshow(decoded_imgs[i].reshape(28, 28), cmap='gray')\n",
    "    plt.axis('off')\n",
    "plt.show()\n",
    "print(\"Visualization complete.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0c394c0-9b37-427c-b816-21336cbc9243",
   "metadata": {},
   "source": [
    "# general\n",
    "\n",
    "1. Regarding the expansion to 1000 dimensions:\n",
    "- This is an example of the \"bottleneck trick\" or \"overcomplete representation\"\n",
    "- By first expanding the dimensions before compression, we allow the network to:\n",
    "  - Learn more robust feature representations\n",
    "  - Better separate different patterns in higher dimensional space\n",
    "  - Create a richer intermediate representation that helps prevent the network from learning just an identity function\n",
    "  - Handle non-linear relationships more effectively\n",
    "\n",
    "2. Using only two hidden layers would:\n",
    "- Reduce the network's capacity to learn complex patterns\n",
    "- Make it harder for the network to create meaningful separations in the latent space\n",
    "- Result in more overlapped clusters in the 2D visualization\n",
    "- Likely show less distinct separation between different digits\n",
    "\n",
    "3. Changing to Leaky ReLU:\n",
    "- Would help prevent \"dying ReLU\" problem where neurons can get stuck in a negative state\n",
    "- Allows for small negative gradients instead of zero\n",
    "- Could potentially lead to:\n",
    "  - Better gradient flow\n",
    "  - More active neurons\n",
    "  - Slightly different cluster formations in the latent space\n",
    "- To try this, you could modify the activation parameter in the code to:\n",
    "```python\n",
    "activation=layers.LeakyReLU(alpha=0.01)\n",
    "```\n",
    "\n",
    "4. Drawbacks of this type of autoencoder:\n",
    "- Loses spatial information due to flattening the images\n",
    "- No explicit preservation of local image structure\n",
    "- Can be sensitive to rotation and translation\n",
    "- May learn inefficient representations due to full connectivity\n",
    "- High number of parameters due to dense layers\n",
    "- Limited to fixed-size inputs\n",
    "\n",
    "5. Regarding unique solutions:\n",
    "- No, autoencoders typically don't have unique solutions\n",
    "- Different random initializations can lead to:\n",
    "  - Different weight configurations\n",
    "  - Different latent space representations\n",
    "  - Similar reconstruction quality\n",
    "- The non-uniqueness comes from:\n",
    "  - Non-convex optimization landscape\n",
    "  - Multiple ways to encode the same information\n",
    "  - Potential symmetries in the network architecture\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
