{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "程式參考並修改自:\n",
    "> https://towardsdatascience.com/how-to-generate-music-using-a-lstm-neural-network-in-keras-68786834d4c5  \n",
    "\n",
    "解釋程式如何改進之參考文章:\n",
    "> https://david-exiga.medium.com/music-generation-using-lstm-neural-networks-44f6780a4c5  \n",
    "\n",
    "額外的中文程式解釋:\n",
    "> https://github.com/xitu/gold-miner/blob/master/TODO1/how-to-generate-music-using-a-lstm-neural-network-in-keras.md"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Collecting music21\n",
      "  Downloading music21-9.3.0-py3-none-any.whl.metadata (5.0 kB)\n",
      "Requirement already satisfied: chardet in /usr/lib/python3/dist-packages (from music21) (5.2.0)\n",
      "Requirement already satisfied: joblib in /home/azzar/.local/lib/python3.12/site-packages (from music21) (1.4.2)\n",
      "Collecting jsonpickle (from music21)\n",
      "  Downloading jsonpickle-4.0.0-py3-none-any.whl.metadata (8.2 kB)\n",
      "Requirement already satisfied: matplotlib in /home/azzar/.local/lib/python3.12/site-packages (from music21) (3.9.2)\n",
      "Requirement already satisfied: more-itertools in /usr/lib/python3/dist-packages (from music21) (10.3.0)\n",
      "Requirement already satisfied: numpy in /home/azzar/.local/lib/python3.12/site-packages (from music21) (1.26.4)\n",
      "Requirement already satisfied: requests in /home/azzar/.local/lib/python3.12/site-packages (from music21) (2.32.3)\n",
      "Requirement already satisfied: webcolors>=1.5 in /home/azzar/.local/lib/python3.12/site-packages (from music21) (24.8.0)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /home/azzar/.local/lib/python3.12/site-packages (from matplotlib->music21) (1.3.0)\n",
      "Requirement already satisfied: cycler>=0.10 in /home/azzar/.local/lib/python3.12/site-packages (from matplotlib->music21) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /home/azzar/.local/lib/python3.12/site-packages (from matplotlib->music21) (4.53.1)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in /home/azzar/.local/lib/python3.12/site-packages (from matplotlib->music21) (1.4.7)\n",
      "Requirement already satisfied: packaging>=20.0 in /home/azzar/.local/lib/python3.12/site-packages (from matplotlib->music21) (24.1)\n",
      "Requirement already satisfied: pillow>=8 in /home/azzar/.local/lib/python3.12/site-packages (from matplotlib->music21) (10.4.0)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in /usr/lib/python3/dist-packages (from matplotlib->music21) (3.1.2)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /home/azzar/.local/lib/python3.12/site-packages (from matplotlib->music21) (2.9.0.post0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /home/azzar/.local/lib/python3.12/site-packages (from requests->music21) (3.4.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/azzar/.local/lib/python3.12/site-packages (from requests->music21) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/azzar/.local/lib/python3.12/site-packages (from requests->music21) (2.2.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/azzar/.local/lib/python3.12/site-packages (from requests->music21) (2024.8.30)\n",
      "Requirement already satisfied: six>=1.5 in /usr/lib/python3/dist-packages (from python-dateutil>=2.7->matplotlib->music21) (1.16.0)\n",
      "Downloading music21-9.3.0-py3-none-any.whl (22.9 MB)\n",
      "\u001b[2K   \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m22.9/22.9 MB\u001b[0m \u001b[31m5.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0mm eta \u001b[36m0:00:01\u001b[0m0:01\u001b[0m:01\u001b[0m\n",
      "\u001b[?25hDownloading jsonpickle-4.0.0-py3-none-any.whl (46 kB)\n",
      "Installing collected packages: jsonpickle, music21\n",
      "Successfully installed jsonpickle-4.0.0 music21-9.3.0\n"
     ]
    }
   ],
   "source": [
    "# 安裝 music21  # music21 介紹: https://juejin.cn/post/7063827463058489352\n",
    "! pip install music21 --break-system-packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 讀取檔案用\n",
    "import glob\n",
    "# array processing\n",
    "import numpy\n",
    "from matplotlib import pyplot\n",
    "# keras for building deep learning model\n",
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, TimeDistributed\n",
    "from keras.layers import Dropout\n",
    "from keras.layers import LSTM\n",
    "from keras.layers import Activation\n",
    "from keras.layers import BatchNormalization as BatchNorm\n",
    "from keras.utils import to_categorical\n",
    "from keras.callbacks import ModelCheckpoint\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 從目錄下的 midi 文件中獲取所有的音符和和弦"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "## \"\"\" 從目錄下的 midi 文件中獲取所有的音符和和弦 \"\"\"\n",
    "\n",
    "# 使用music21來進行midi檔案的操作\n",
    "from music21 import converter, instrument, note, chord, stream\n",
    "# music21 介紹: https://juejin.cn/post/7063827463058489352\n",
    "\n",
    "\n",
    "# 使用glob讀取midi檔案，路徑代號為:\n",
    "# \"./\"               -使用當前執行程式所在的資料夾\n",
    "# \"midi_songs/\"      -名為midi_songs資料夾\n",
    "# \"*.mid\"            -所有結尾為的.mid檔案\n",
    "\n",
    "\n",
    "# note是音符，將midi檔案裡的音符讀進這個list\n",
    "notes = []\n",
    "\n",
    "for file in glob.glob(\"./midi_songs/*.mid\"): # 讀取目錄檔案路徑中所有midi檔案\n",
    "    \n",
    "    # 使用 music21 解析midi文件\n",
    "    midi = converter.parse(file)\n",
    "\n",
    "    print(\"Parsing %s\" % file)\n",
    "\n",
    "    notes_to_parse = None\n",
    "\n",
    "    try: #如果有樂器部分，取第一個樂器\n",
    "        s2 = instrument.partitionByInstrument(midi)\n",
    "        notes_to_parse = s2.parts[0].recurse() \n",
    "    except: #如果沒有樂器部分，直接取note\n",
    "        notes_to_parse = midi.flat.notes\n",
    "\n",
    "    for element in notes_to_parse:\n",
    "        #print(element)\n",
    "        # 如果是 Note 型別，取音調\n",
    "        if isinstance(element, note.Note):\n",
    "            notes.append(str(element.pitch))\n",
    "        # 如果是 Chord 型別，取音調的序號,存int型別比較容易處理\n",
    "        elif isinstance(element, chord.Chord):\n",
    "            notes.append('.'.join(str(n) for n in element.normalOrder))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 準備神經網絡使用的輸入輸出"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===== 項目解釋 ======\n",
      "\n",
      "notes: 是一個(list)，當中以(字串)儲存所有樂譜的音符\n",
      "樂譜裡的所有音符總共有: 0 個 \n",
      "樂譜裡[音符種類]共有: 0 個\n",
      "音符種類名稱分別是: []\n",
      "音符種類與對應的號碼: {} \n",
      "\n",
      "===================\n",
      "\n",
      "notes: \t共有: 0 項字串\n",
      "notes 中的每 100 項音符轉換成一組訓練資料 \n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[13], line 36\u001b[0m\n\u001b[1;32m     34\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnotes: \u001b[39m\u001b[38;5;130;01m\\t\u001b[39;00m\u001b[38;5;124m共有: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(notes)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m 項字串\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     35\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnotes 中的每 \u001b[39m\u001b[38;5;132;01m{\u001b[39;00msequence_length\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m 項音符轉換成一組訓練資料 \u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 36\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnetwork_input:  共有: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(network_input)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m 組list，每組裡有: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(\u001b[43mnetwork_input\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m 項數字\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     37\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnetwork_output: 共有: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(network_output)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m 項數字，每項都是input的再後面一項的音符數字\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     38\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m===================\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mIndexError\u001b[0m: list index out of range"
     ]
    }
   ],
   "source": [
    "## \"\"\" 準備神經網絡使用的輸入輸出 \"\"\"\n",
    "\n",
    "# 獲取音符種類名稱的數量\n",
    "n_vocab = len(set(notes))\n",
    "# 獲得排序後的音符種類名稱\n",
    "pitchnames = sorted(set(item for item in notes))\n",
    "# 創建一個字典，把每個音符轉換分配一個對應的數字號碼 ex:(C4 > 25)，利於訓練\n",
    "note_to_int = dict((note, number) for number, note in enumerate(pitchnames))\n",
    "\n",
    "print(\"\\n===== 項目解釋 ======\\n\")\n",
    "print(\"notes: 是一個(list)，當中以(字串)儲存所有樂譜的音符\")\n",
    "print(f\"樂譜裡的所有音符總共有: %d 個 \"         % len(notes))\n",
    "print(f\"樂譜裡[音符種類]共有: %d 個\"      %  n_vocab)\n",
    "print(f\"音符種類名稱分別是: %s\"            %  pitchnames)\n",
    "print(f\"音符種類與對應的號碼: {note_to_int} \"  )\n",
    "\n",
    "\n",
    "\n",
    "# 訓練輸入序列的長度(輸入音符的個數) \n",
    "sequence_length = 100 \n",
    "\n",
    "network_input = [] #創建輸入序列\n",
    "network_output = [] #創建輸出序列\n",
    "\n",
    "# =====使用notes裡的音符創建輸入序列和相應的輸出=====\n",
    "for i in range(0, len(notes) - sequence_length, 1):\n",
    "    sequence_in = notes[i:i + sequence_length]  # [0 ~ length-1 項音符], [1 ~ length 項音符], [1 ~ length+1項音符]....\n",
    "    sequence_out = notes[i + sequence_length]   # 第length項音符, 第length + 1項音符,第length + 2項音符...\n",
    "\n",
    "    network_input.append([note_to_int[char] for char in sequence_in]) # 加入, 把sequence_in裡的音符翻譯成的號碼\n",
    "    network_output.append(note_to_int[sequence_out])    # 作為預測用的 label\n",
    "\n",
    "print(\"\\n===================\\n\")\n",
    "print(f\"notes: \\t共有: {len(notes)} 項字串\")\n",
    "print(f\"notes 中的每 {sequence_length} 項音符轉換成一組訓練資料 \")\n",
    "print(f\"network_input:  共有: {len(network_input)} 組list，每組裡有: {len(network_input[0])} 項數字\")\n",
    "print(f\"network_output: 共有: {len(network_output)} 項數字，每項都是input的再後面一項的音符數字\")\n",
    "print(\"\\n===================\\n\")\n",
    "print(\"notes 第 sequence_length -10 往後10項 音符名稱分別是:\\t\",notes[sequence_length-10:sequence_length])\n",
    "print(\"notes 第 sequence_length -10 往後10項 音符對應的數字是:\\t\", [note_to_int[char] for char in notes[sequence_length-10:sequence_length]])\n",
    "print(\"\")\n",
    "print(f\"network_input 第0組 的 最後10 項號碼:\\t\",network_input[0][sequence_length-10:sequence_length])\n",
    "print(f\"network_input 第1組 的 最後10 項號碼:\\t\",network_input[1][sequence_length-10:sequence_length])\n",
    "print(f\"network_input 第2組 的 最後10 項號碼:\\t\",network_input[2][sequence_length-10:sequence_length])\n",
    "print(\"network_output 的第 0~2 項號碼:\",network_output[0:3])\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "n_patterns = len(network_input)\n",
    "# =====將輸入重塑為與 LSTM 層兼容的格式=====\n",
    "normalized_input = numpy.reshape(network_input, (n_patterns, sequence_length, 1))\n",
    "\n",
    "# 正規化輸入\n",
    "normalized_input = normalized_input / float(n_vocab)\n",
    "#輸出bool矩陣，以n_vocab維度表示一個數字，用以配合categorical_crossentropy 算法\n",
    "# to_categorical解釋: https://blog.csdn.net/moyu123456789/article/details/83444140\n",
    "network_output = to_categorical(network_output,n_vocab)\n",
    "\n",
    "print(\"\\n===== 資料重塑後 =========\\n\")\n",
    "print(\"normalized_input.shape:\",normalized_input.shape)\n",
    "print(\"network_output.shape:\",network_output.shape)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 創建神經網絡的結構 \n",
    "### LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'normalized_input' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[14], line 9\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m##\"\"\" 創建神經網絡的結構 \"\"\"\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;66;03m#LSTM\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \n\u001b[1;32m      4\u001b[0m \n\u001b[1;32m      5\u001b[0m \u001b[38;5;66;03m# https://huhuhang.com/post/machine-learning/lstm-return-sequences-state\u001b[39;00m\n\u001b[1;32m      6\u001b[0m model \u001b[38;5;241m=\u001b[39m Sequential()\n\u001b[1;32m      7\u001b[0m model\u001b[38;5;241m.\u001b[39madd(LSTM(\n\u001b[1;32m      8\u001b[0m         \u001b[38;5;241m512\u001b[39m,\n\u001b[0;32m----> 9\u001b[0m         input_shape\u001b[38;5;241m=\u001b[39m(\u001b[43mnormalized_input\u001b[49m\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m], normalized_input\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m2\u001b[39m]),\n\u001b[1;32m     10\u001b[0m         recurrent_dropout\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.1\u001b[39m,\n\u001b[1;32m     11\u001b[0m         return_sequences\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m     12\u001b[0m     ))\n\u001b[1;32m     13\u001b[0m model\u001b[38;5;241m.\u001b[39madd(LSTM(\u001b[38;5;241m512\u001b[39m, return_sequences\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, recurrent_dropout\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.1\u001b[39m,))\n\u001b[1;32m     14\u001b[0m model\u001b[38;5;241m.\u001b[39madd(LSTM(\u001b[38;5;241m512\u001b[39m))\n",
      "\u001b[0;31mNameError\u001b[0m: name 'normalized_input' is not defined"
     ]
    }
   ],
   "source": [
    "##\"\"\" 創建神經網絡的結構 \"\"\"\n",
    "#LSTM\n",
    "\n",
    "\n",
    "# https://huhuhang.com/post/machine-learning/lstm-return-sequences-state\n",
    "model = Sequential()\n",
    "model.add(LSTM(\n",
    "        512,\n",
    "        input_shape=(normalized_input.shape[1], normalized_input.shape[2]),\n",
    "        recurrent_dropout=0.1,\n",
    "        return_sequences=True\n",
    "    ))\n",
    "model.add(LSTM(512, return_sequences=True, recurrent_dropout=0.1,))\n",
    "model.add(LSTM(512))\n",
    "model.add(BatchNorm())\n",
    "model.add(Dropout(0.1))\n",
    "model.add(Dense(256))\n",
    "model.add(Activation('relu'))\n",
    "model.add(BatchNorm())\n",
    "model.add(Dropout(0.1))\n",
    "model.add(Dense(n_vocab))\n",
    "model.add(Activation('softmax'))\n",
    "model.compile(loss='categorical_crossentropy', optimizer='rmsprop')\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 訓練神經網絡"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'normalized_input' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 4\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m## \"\"\" 訓練神經網絡 \"\"\"\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;66;03m# callbacks = ModelCheckpoint('model{epoch:03d}.weights.h5', save_weights_only=True)\u001b[39;00m\n\u001b[0;32m----> 4\u001b[0m model\u001b[38;5;241m.\u001b[39mfit(\u001b[43mnormalized_input\u001b[49m, network_output, epochs\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m, batch_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m128\u001b[39m ,callbacks\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'normalized_input' is not defined"
     ]
    }
   ],
   "source": [
    "## \"\"\" 訓練神經網絡 \"\"\"\n",
    "# callbacks = ModelCheckpoint('model{epoch:03d}.weights.h5', save_weights_only=True)\n",
    "\n",
    "model.fit(normalized_input, network_output, epochs=1, batch_size=128 ,callbacks=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "根據選定的音符起始點，從神經網絡預測下一個音符並生成樂譜"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "high <= 0",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 4\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m##\"\"\" 根據選定的音符起始點，從神經網絡預測下一個音符並生成樂譜 \"\"\"\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m# 從 network_input 中選擇一個組隨機序列作為預測的起點\u001b[39;00m\n\u001b[0;32m----> 4\u001b[0m start \u001b[38;5;241m=\u001b[39m \u001b[43mnumpy\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrandom\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrandint\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mnetwork_input\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;66;03m# 作為預測 起始點的一串音符\u001b[39;00m\n\u001b[1;32m      6\u001b[0m pattern \u001b[38;5;241m=\u001b[39m network_input[start]\n",
      "File \u001b[0;32mnumpy/random/mtrand.pyx:782\u001b[0m, in \u001b[0;36mnumpy.random.mtrand.RandomState.randint\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mnumpy/random/_bounded_integers.pyx:1334\u001b[0m, in \u001b[0;36mnumpy.random._bounded_integers._rand_int64\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: high <= 0"
     ]
    }
   ],
   "source": [
    "##\"\"\" 根據選定的音符起始點，從神經網絡預測下一個音符並生成樂譜 \"\"\"\n",
    "\n",
    "# 從 network_input 中選擇一個組隨機序列作為預測的起點\n",
    "start = numpy.random.randint(0, len(network_input)-1)\n",
    "# 作為預測 起始點的一串音符\n",
    "pattern = network_input[start]\n",
    "\n",
    "# 把數字還原回音符的字典\n",
    "int_to_note = dict((number, note) for number, note in enumerate(pitchnames)) \n",
    "\n",
    "\n",
    "prediction_output = []\n",
    "\n",
    "print(\"生成的音符:\")\n",
    "\n",
    "# 隨機生成n個音符\n",
    "for note_index in range(100):#更改range()改變音符生成的數量\n",
    "    prediction_input = numpy.reshape(pattern, (1, len(pattern), 1))\n",
    "    prediction_input = prediction_input / float(n_vocab) #正規化\n",
    "\n",
    "    #預測每一個音符的概率 \n",
    "    prediction = model.predict(prediction_input, verbose=0) \n",
    "\n",
    "\n",
    "    #挑選prediction_output裡最大的值\n",
    "    index = numpy.argmax(prediction)\n",
    "\n",
    "    #提取對應的音符\n",
    "    result = int_to_note[index]\n",
    "    print(index)\n",
    "    prediction_output.append(result)\n",
    "\n",
    "    # 將預測的一個音符放入預測窗口sequence_length，放掉原本窗口內最左邊的一個音符(窗口往右移動)\n",
    "    pattern.append(index)\n",
    "    pattern = pattern[1:len(pattern)]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "將預測的輸出轉換為音符，並從音符中創建一個MIDI文件 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'prediction_output' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[9], line 7\u001b[0m\n\u001b[1;32m      4\u001b[0m output_notes \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m      6\u001b[0m \u001b[38;5;66;03m# 根據模型生成的值創建音符和和弦對象\u001b[39;00m\n\u001b[0;32m----> 7\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m pattern \u001b[38;5;129;01min\u001b[39;00m \u001b[43mprediction_output\u001b[49m:\n\u001b[1;32m      8\u001b[0m     \u001b[38;5;66;03m# 模式是和弦\u001b[39;00m\n\u001b[1;32m      9\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m (\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;129;01min\u001b[39;00m pattern) \u001b[38;5;129;01mor\u001b[39;00m pattern\u001b[38;5;241m.\u001b[39misdigit():\n\u001b[1;32m     10\u001b[0m         notes_in_chord \u001b[38;5;241m=\u001b[39m pattern\u001b[38;5;241m.\u001b[39msplit(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'prediction_output' is not defined"
     ]
    }
   ],
   "source": [
    "## \"\"\" 將預測的輸出轉換為音符，並從音符中創建一個MIDI文件 \"\"\"\n",
    "\n",
    "offset = 0\n",
    "output_notes = []\n",
    "\n",
    "# 根據模型生成的值創建音符和和弦對象\n",
    "for pattern in prediction_output:\n",
    "    # 模式是和弦\n",
    "    if ('.' in pattern) or pattern.isdigit():\n",
    "        notes_in_chord = pattern.split('.')\n",
    "        notes = []\n",
    "        for current_note in notes_in_chord:\n",
    "            \n",
    "            try:\n",
    "                new_note = note.Note(int(current_note))\n",
    "#                 print(new_note)\n",
    "\n",
    "            except ValueError:\n",
    "                   pass \n",
    "                \n",
    "            new_note.storedInstrument = instrument.Piano()\n",
    "            notes.append(new_note)\n",
    "        new_chord = chord.Chord(notes)\n",
    "        new_chord.offset = offset\n",
    "        output_notes.append(new_chord)\n",
    "    # 模式是一個音符\n",
    "    else:\n",
    "        new_note = note.Note(pattern)\n",
    "        new_note.offset = offset\n",
    "        new_note.storedInstrument = instrument.Piano()\n",
    "        output_notes.append(new_note)\n",
    "\n",
    "    # 音符之間的間距\n",
    "    offset += 0.5\n",
    "    \n",
    "# 創建樂譜\n",
    "midi_stream = stream.Stream(output_notes)\n",
    "\n",
    "# 創建 MIDI 文件\n",
    "midi_stream.write('midi', fp='LAB3A_LSTM_music.mid')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
